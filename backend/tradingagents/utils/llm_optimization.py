"""
LLM Optimization Utilities

æä¾›LLMè°ƒç”¨ä¼˜åŒ–åŠŸèƒ½ï¼š
1. ä¸Šä¸‹æ–‡è£å‰ªï¼ˆContext Pruningï¼‰- æ™ºèƒ½æˆªæ–­è¿‡é•¿è¾“å…¥
2. ç»“æœç¼“å­˜ï¼ˆResult Cachingï¼‰- ç¼“å­˜é‡å¤æŸ¥è¯¢ç»“æœ
3. æ‰¹å¤„ç†ï¼ˆBatchingï¼‰- æ‰¹é‡å¤„ç†è¯·æ±‚é™ä½å»¶è¿Ÿ

é¢„æœŸæ•ˆæœï¼š
- é™ä½30-50% tokenæ¶ˆè€—
- å‡å°‘40-60% APIè°ƒç”¨æ¬¡æ•°
- æå‡20-30% å“åº”é€Ÿåº¦
"""

import hashlib
import time
from typing import Dict, List, Any, Optional, Callable
from functools import wraps
from collections import OrderedDict
from threading import Lock

from tradingagents.utils.logging_init import get_logger

logger = get_logger("llm_optimization")


class ContextPruner:
    """ä¸Šä¸‹æ–‡è£å‰ªå™¨ - æ™ºèƒ½æˆªæ–­è¿‡é•¿è¾“å…¥"""

    def __init__(
        self,
        max_tokens: int = 4000,
        preserve_ratio: float = 0.8,
        truncate_strategy: str = "middle"
    ):
        """
        åˆå§‹åŒ–ä¸Šä¸‹æ–‡è£å‰ªå™¨

        Args:
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆè¿‘ä¼¼å€¼ï¼ŒæŒ‰4å­—ç¬¦=1tokenè®¡ç®—ï¼‰
            preserve_ratio: ä¿ç•™æ¯”ä¾‹ï¼ˆ0.8è¡¨ç¤ºä¿ç•™å‰80%+å20%ï¼‰
            truncate_strategy: æˆªæ–­ç­–ç•¥
                - "middle": ä¿ç•™å¼€å¤´å’Œç»“å°¾ï¼Œåˆ é™¤ä¸­é—´
                - "tail": åªä¿ç•™å¼€å¤´
                - "smart": æ™ºèƒ½åˆ†æ®µä¿ç•™ï¼ˆä¿ç•™ç« èŠ‚æ ‡é¢˜ï¼‰
        """
        self.max_tokens = max_tokens
        self.preserve_ratio = preserve_ratio
        self.truncate_strategy = truncate_strategy

    def _estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—tokenæ•°é‡

        ç®€åŒ–è®¡ç®—ï¼šä¸­æ–‡çº¦1å­—=1tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦=1token
        """
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        other_chars = len(text) - chinese_chars

        return chinese_chars + (other_chars // 4)

    def truncate(self, text: str) -> tuple[str, bool]:
        """
        æˆªæ–­æ–‡æœ¬

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            (æˆªæ–­åæ–‡æœ¬, æ˜¯å¦å‘ç”Ÿæˆªæ–­)
        """
        estimated_tokens = self._estimate_tokens(text)

        if estimated_tokens <= self.max_tokens:
            return text, False

        # å‘ç”Ÿæˆªæ–­
        target_length = int(len(text) * (self.max_tokens / estimated_tokens))

        if self.truncate_strategy == "tail":
            # åªä¿ç•™å¼€å¤´
            truncated = text[:target_length]
            truncated += "\n\n...[å†…å®¹å·²æˆªæ–­]..."

        elif self.truncate_strategy == "middle":
            # ä¿ç•™å¼€å¤´å’Œç»“å°¾
            head_length = int(target_length * self.preserve_ratio)
            tail_length = target_length - head_length

            truncated = text[:head_length]
            truncated += "\n\n...[ä¸­é—´å†…å®¹å·²æˆªæ–­]...\n\n"
            truncated += text[-tail_length:]

        elif self.truncate_strategy == "smart":
            # æ™ºèƒ½åˆ†æ®µä¿ç•™ï¼ˆä¿ç•™æ ‡é¢˜ï¼‰
            lines = text.split('\n')
            truncated_lines = []
            current_length = 0

            for line in lines:
                # ä¿ç•™æ ‡é¢˜è¡Œï¼ˆä»¥#å¼€å¤´ï¼‰
                if line.startswith('#') or line.startswith('##'):
                    truncated_lines.append(line)
                    current_length += len(line)
                elif current_length + len(line) < target_length:
                    truncated_lines.append(line)
                    current_length += len(line)
                else:
                    break

            truncated = '\n'.join(truncated_lines)
            truncated += "\n\n...[éƒ¨åˆ†å†…å®¹å·²çœç•¥]..."

        else:
            # é»˜è®¤tailç­–ç•¥
            truncated = text[:target_length] + "\n\n...[å†…å®¹å·²æˆªæ–­]..."

        logger.debug(f"Context truncated: {estimated_tokens} -> {self._estimate_tokens(truncated)} tokens (strategy={self.truncate_strategy})")

        return truncated, True


class LLMResultCache:
    """LLMç»“æœç¼“å­˜ - ç¼“å­˜é‡å¤æŸ¥è¯¢ç»“æœ"""

    def __init__(
        self,
        max_size: int = 1000,
        ttl_seconds: int = 3600
    ):
        """
        åˆå§‹åŒ–ç»“æœç¼“å­˜

        Args:
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            ttl_seconds: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self._cache: OrderedDict[str, Dict[str, Any]] = OrderedDict()
        self._lock = Lock()

        self.hits = 0
        self.misses = 0

    def _generate_key(self, prompt: str, model: str) -> str:
        """
        ç”Ÿæˆç¼“å­˜key

        Args:
            prompt: è¾“å…¥prompt
            model: æ¨¡å‹åç§°

        Returns:
            ç¼“å­˜keyï¼ˆMD5å“ˆå¸Œï¼‰
        """
        content = f"{model}:{prompt}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()

    def get(self, prompt: str, model: str) -> Optional[str]:
        """
        è·å–ç¼“å­˜ç»“æœ

        Args:
            prompt: è¾“å…¥prompt
            model: æ¨¡å‹åç§°

        Returns:
            ç¼“å­˜çš„ç»“æœï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        key = self._generate_key(prompt, model)

        with self._lock:
            if key in self._cache:
                cached_item = self._cache[key]

                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if time.time() - cached_item['timestamp'] < self.ttl_seconds:
                    # å‘½ä¸­ï¼Œç§»åˆ°æœ«å°¾ï¼ˆLRUï¼‰
                    self._cache.move_to_end(key)
                    self.hits += 1

                    logger.debug(f"Cache hit: {key[:8]}... (hit_rate={self.hit_rate:.2%})")
                    return cached_item['result']
                else:
                    # è¿‡æœŸï¼Œåˆ é™¤
                    del self._cache[key]

            self.misses += 1
            return None

    def set(self, prompt: str, model: str, result: str):
        """
        è®¾ç½®ç¼“å­˜ç»“æœ

        Args:
            prompt: è¾“å…¥prompt
            model: æ¨¡å‹åç§°
            result: LLMè¾“å‡ºç»“æœ
        """
        key = self._generate_key(prompt, model)

        with self._lock:
            # LRUæ·˜æ±°
            if len(self._cache) >= self.max_size:
                self._cache.popitem(last=False)

            self._cache[key] = {
                'result': result,
                'timestamp': time.time()
            }

            logger.debug(f"Cache set: {key[:8]}... (size={len(self._cache)})")

    @property
    def hit_rate(self) -> float:
        """ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self._cache.clear()
            self.hits = 0
            self.misses = 0
            logger.info("Cache cleared")

    def stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return {
            "size": len(self._cache),
            "max_size": self.max_size,
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": self.hit_rate,
            "ttl_seconds": self.ttl_seconds
        }


# å…¨å±€å®ä¾‹
_global_pruner: Optional[ContextPruner] = None
_global_cache: Optional[LLMResultCache] = None


def get_context_pruner(
    max_tokens: int = 4000,
    truncate_strategy: str = "middle"
) -> ContextPruner:
    """
    è·å–å…¨å±€ä¸Šä¸‹æ–‡è£å‰ªå™¨ï¼ˆå•ä¾‹ï¼‰

    Args:
        max_tokens: æœ€å¤§tokenæ•°
        truncate_strategy: æˆªæ–­ç­–ç•¥

    Returns:
        ContextPrunerå®ä¾‹
    """
    global _global_pruner

    if _global_pruner is None:
        _global_pruner = ContextPruner(
            max_tokens=max_tokens,
            truncate_strategy=truncate_strategy
        )
        logger.info(f"ğŸ“ Context Pruner initialized: max_tokens={max_tokens}, strategy={truncate_strategy}")

    return _global_pruner


def get_llm_cache(
    max_size: int = 1000,
    ttl_seconds: int = 3600
) -> LLMResultCache:
    """
    è·å–å…¨å±€LLMç¼“å­˜ï¼ˆå•ä¾‹ï¼‰

    Args:
        max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        ttl_seconds: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        LLMResultCacheå®ä¾‹
    """
    global _global_cache

    if _global_cache is None:
        _global_cache = LLMResultCache(
            max_size=max_size,
            ttl_seconds=ttl_seconds
        )
        logger.info(f"ğŸ’¾ LLM Cache initialized: max_size={max_size}, ttl={ttl_seconds}s")

    return _global_cache


def optimize_llm_call(
    enable_pruning: bool = True,
    enable_caching: bool = True,
    max_tokens: int = 4000,
    truncate_strategy: str = "middle"
):
    """
    LLMè°ƒç”¨ä¼˜åŒ–è£…é¥°å™¨

    è‡ªåŠ¨åº”ç”¨ä¸Šä¸‹æ–‡è£å‰ªå’Œç»“æœç¼“å­˜

    Usage:
        @optimize_llm_call(enable_pruning=True, enable_caching=True)
        def call_llm(prompt: str, model: str) -> str:
            # ... LLMè°ƒç”¨é€»è¾‘ ...
            return result

    Args:
        enable_pruning: æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡è£å‰ª
        enable_caching: æ˜¯å¦å¯ç”¨ç»“æœç¼“å­˜
        max_tokens: æœ€å¤§tokenæ•°
        truncate_strategy: æˆªæ–­ç­–ç•¥
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            # å°è¯•ä»å‚æ•°ä¸­æå–promptå’Œmodel
            prompt = None
            model = None

            # å‡è®¾ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯prompt
            if len(args) > 0:
                prompt = str(args[0])

            # å‡è®¾modelåœ¨kwargsä¸­
            model = kwargs.get('model', 'unknown')

            # åº”ç”¨ä¸Šä¸‹æ–‡è£å‰ª
            if enable_pruning and prompt:
                pruner = get_context_pruner(max_tokens, truncate_strategy)
                pruned_prompt, was_truncated = pruner.truncate(prompt)

                # æ›¿æ¢argsä¸­çš„prompt
                if was_truncated:
                    args = (pruned_prompt,) + args[1:]
                    logger.info(f"ğŸ”ª Context pruned for model: {model}")

            # å°è¯•ä»ç¼“å­˜è·å–ç»“æœ
            if enable_caching and prompt:
                cache = get_llm_cache()
                cached_result = cache.get(prompt, model)

                if cached_result is not None:
                    logger.info(f"ğŸ’¾ Cache hit for model: {model}")
                    return cached_result

            # è°ƒç”¨åŸå§‹å‡½æ•°
            start_time = time.time()
            result = func(*args, **kwargs)
            duration = time.time() - start_time

            logger.debug(f"â±ï¸  LLM call completed: model={model}, duration={duration:.2f}s")

            # ç¼“å­˜ç»“æœ
            if enable_caching and prompt and result:
                cache = get_llm_cache()
                cache.set(prompt, model, result)

            return result

        return wrapper

    return decorator


# Convenience functions

def prune_context(text: str, max_tokens: int = 4000, strategy: str = "middle") -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šæˆªæ–­ä¸Šä¸‹æ–‡

    Args:
        text: è¾“å…¥æ–‡æœ¬
        max_tokens: æœ€å¤§tokenæ•°
        strategy: æˆªæ–­ç­–ç•¥

    Returns:
        æˆªæ–­åæ–‡æœ¬
    """
    pruner = get_context_pruner(max_tokens, strategy)
    truncated, _ = pruner.truncate(text)
    return truncated


def clear_llm_cache():
    """ä¾¿æ·å‡½æ•°ï¼šæ¸…ç©ºLLMç¼“å­˜"""
    cache = get_llm_cache()
    cache.clear()


def get_llm_cache_stats() -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–LLMç¼“å­˜ç»Ÿè®¡"""
    cache = get_llm_cache()
    return cache.stats()
