"""
LLM Router - Multi-tier LLM selection with fallback mechanism

æä¾›ä¸‰å±‚LLMè·¯ç”±ç­–ç•¥:
1. Small models (Qwen-Turbo, Qwen-7B) - ç®€å•åˆ†æä»»åŠ¡
2. Medium models (Qwen-Plus, GPT-4o-mini) - å¸¸è§„åˆ†æä»»åŠ¡
3. Large models (DeepSeek, Claude, GPT-4) - å¤æ‚æ¨ç†ä»»åŠ¡

æ”¯æŒè‡ªåŠ¨é™çº§/å‡çº§æœºåˆ¶ï¼Œç¡®ä¿ä»»åŠ¡å®Œæˆè´¨é‡ã€‚
"""

import os
from typing import Dict, Any, Optional, Literal
from enum import Enum
from langchain_openai import ChatOpenAI

from tradingagents.utils.logging_init import get_logger
logger = get_logger("llm_router")


class LLMTier(str, Enum):
    """LLMæ¨¡å‹å±‚çº§"""
    SMALL = "small"      # å¿«é€Ÿç®€å•ä»»åŠ¡ï¼ˆå¦‚æ•°æ®æ ¼å¼åŒ–ã€æ¨¡æ¿å¡«å……ï¼‰
    MEDIUM = "medium"    # å¸¸è§„åˆ†æä»»åŠ¡ï¼ˆå¦‚æŠ€æœ¯æŒ‡æ ‡åˆ†æã€åŸºæœ¬é¢æŠ¥å‘Šï¼‰
    LARGE = "large"      # å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚å¤šæ–¹è¾©è®ºã€é£é™©å†³ç­–ï¼‰


class AgentComplexity(str, Enum):
    """Agentä»»åŠ¡å¤æ‚åº¦åˆ†ç±»"""
    SIMPLE = "simple"       # ç®€å•ä»»åŠ¡
    ROUTINE = "routine"     # å¸¸è§„åˆ†æ
    COMPLEX = "complex"     # å¤æ‚æ¨ç†


# Agentç±»å‹åˆ°å¤æ‚åº¦çš„æ˜ å°„
AGENT_COMPLEXITY_MAP: Dict[str, AgentComplexity] = {
    # åˆ†æå¸ˆ - å¸¸è§„åˆ†æ
    "market": AgentComplexity.ROUTINE,
    "fundamentals": AgentComplexity.ROUTINE,
    "social": AgentComplexity.ROUTINE,
    "news": AgentComplexity.ROUTINE,

    # ç ”ç©¶å‘˜ - å¸¸è§„åˆ†æï¼ˆæ”¶é›†è§‚ç‚¹ï¼‰
    "bull_researcher": AgentComplexity.ROUTINE,
    "bear_researcher": AgentComplexity.ROUTINE,

    # é£é™©åˆ†æå‘˜ - å¸¸è§„åˆ†æ
    "risky_analyst": AgentComplexity.ROUTINE,
    "neutral_analyst": AgentComplexity.ROUTINE,
    "safe_analyst": AgentComplexity.ROUTINE,

    # äº¤æ˜“å‘˜ - ç®€å•æ‰§è¡Œ
    "trader": AgentComplexity.SIMPLE,

    # ç®¡ç†è€…/è£åˆ¤ - å¤æ‚æ¨ç†
    "research_manager": AgentComplexity.COMPLEX,
    "risk_manager": AgentComplexity.COMPLEX,
}


class LLMRouter:
    """LLMè·¯ç”±å™¨ - æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–LLMè·¯ç”±å™¨

        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«LLMæä¾›å•†å’Œæ¨¡å‹åç§°
        """
        self.config = config
        self.llm_provider = config.get("llm_provider", "openai").lower()
        self.backend_url = config.get("backend_url", "https://api.openai.com/v1")

        # ä»ç¯å¢ƒå˜é‡è¯»å–ä¸‰å±‚æ¨¡å‹é…ç½®
        self.small_model = os.getenv("SMALL_LLM", config.get("small_llm", "gpt-4o-mini"))
        self.medium_model = os.getenv("MEDIUM_LLM", config.get("quick_think_llm", "gpt-4o-mini"))
        self.large_model = os.getenv("LARGE_LLM", config.get("deep_think_llm", "o4-mini"))

        # æ˜¯å¦å¯ç”¨å°æ¨¡å‹è·¯ç”±ï¼ˆé»˜è®¤falseï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
        self.enable_small_model_routing = os.getenv("ENABLE_SMALL_MODEL_ROUTING", "false").lower() == "true"

        # ç¼“å­˜å·²åˆ›å»ºçš„LLMå®ä¾‹
        self._llm_cache: Dict[str, ChatOpenAI] = {}

        logger.info(f"ğŸ¤– LLM Router initialized")
        logger.info(f"   Provider: {self.llm_provider}")
        logger.info(f"   Small model: {self.small_model}")
        logger.info(f"   Medium model: {self.medium_model}")
        logger.info(f"   Large model: {self.large_model}")
        logger.info(f"   Small model routing: {'âœ… Enabled' if self.enable_small_model_routing else 'âŒ Disabled (backward compatible)'}")

    def _create_llm(self, model_name: str, tier: LLMTier) -> ChatOpenAI:
        """
        åˆ›å»ºLLMå®ä¾‹

        Args:
            model_name: æ¨¡å‹åç§°
            tier: æ¨¡å‹å±‚çº§

        Returns:
            ChatOpenAIå®ä¾‹
        """
        # ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤åˆ›å»º
        cache_key = f"{tier}_{model_name}"
        if cache_key in self._llm_cache:
            return self._llm_cache[cache_key]

        # æ ¹æ®tierè®¾ç½®ä¸åŒçš„temperatureå’Œmax_tokens
        if tier == LLMTier.SMALL:
            temperature = 0.1  # å°æ¨¡å‹ä½¿ç”¨ä½temperatureï¼Œæ›´ç¨³å®š
            max_tokens = 1000  # é™åˆ¶è¾“å‡ºé•¿åº¦
        elif tier == LLMTier.MEDIUM:
            temperature = 0.3
            max_tokens = 2000
        else:  # LARGE
            temperature = 0.7  # å¤§æ¨¡å‹ä½¿ç”¨é«˜temperatureï¼Œæ›´æœ‰åˆ›é€ æ€§
            max_tokens = 4000

        llm = ChatOpenAI(
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            base_url=self.backend_url,
            streaming=False
        )

        self._llm_cache[cache_key] = llm
        logger.debug(f"âœ“ Created LLM: {model_name} (tier={tier}, temp={temperature})")

        return llm

    def get_llm_for_agent(
        self,
        agent_type: str,
        fallback_tier: Optional[LLMTier] = None
    ) -> ChatOpenAI:
        """
        æ ¹æ®Agentç±»å‹è·å–åˆé€‚çš„LLM

        Args:
            agent_type: Agentç±»å‹ (market, fundamentals, research_manager, etc.)
            fallback_tier: å¯é€‰çš„é™çº§å±‚çº§ï¼ˆç”¨äºé‡è¯•ï¼‰

        Returns:
            ChatOpenAIå®ä¾‹
        """
        # å¦‚æœæœªå¯ç”¨å°æ¨¡å‹è·¯ç”±ï¼Œä½¿ç”¨ä¼ ç»Ÿé€»è¾‘ï¼ˆå‘åå…¼å®¹ï¼‰
        if not self.enable_small_model_routing:
            complexity = AGENT_COMPLEXITY_MAP.get(agent_type, AgentComplexity.ROUTINE)
            if complexity == AgentComplexity.COMPLEX:
                return self._create_llm(self.large_model, LLMTier.LARGE)
            else:
                return self._create_llm(self.medium_model, LLMTier.MEDIUM)

        # å¯ç”¨å°æ¨¡å‹è·¯ç”±åçš„æ–°é€»è¾‘
        complexity = AGENT_COMPLEXITY_MAP.get(agent_type, AgentComplexity.ROUTINE)

        # å¦‚æœæŒ‡å®šäº†fallback_tierï¼Œä½¿ç”¨é™çº§åçš„tier
        if fallback_tier:
            tier = fallback_tier
            logger.info(f"ğŸ”„ [{agent_type}] Using fallback tier: {tier}")
        else:
            # æ ¹æ®å¤æ‚åº¦é€‰æ‹©tier
            if complexity == AgentComplexity.SIMPLE:
                tier = LLMTier.SMALL
            elif complexity == AgentComplexity.ROUTINE:
                tier = LLMTier.MEDIUM
            else:  # COMPLEX
                tier = LLMTier.LARGE

        # æ ¹æ®tieré€‰æ‹©æ¨¡å‹
        if tier == LLMTier.SMALL:
            model_name = self.small_model
        elif tier == LLMTier.MEDIUM:
            model_name = self.medium_model
        else:  # LARGE
            model_name = self.large_model

        logger.debug(f"ğŸ“‹ [{agent_type}] Routing: complexity={complexity.value}, tier={tier.value}, model={model_name}")

        return self._create_llm(model_name, tier)

    def get_llm_for_complexity(self, complexity: AgentComplexity) -> ChatOpenAI:
        """
        æ ¹æ®ä»»åŠ¡å¤æ‚åº¦ç›´æ¥è·å–LLM

        Args:
            complexity: ä»»åŠ¡å¤æ‚åº¦

        Returns:
            ChatOpenAIå®ä¾‹
        """
        if complexity == AgentComplexity.SIMPLE:
            return self._create_llm(self.small_model, LLMTier.SMALL)
        elif complexity == AgentComplexity.ROUTINE:
            return self._create_llm(self.medium_model, LLMTier.MEDIUM)
        else:
            return self._create_llm(self.large_model, LLMTier.LARGE)

    def upgrade_tier(self, current_tier: LLMTier) -> Optional[LLMTier]:
        """
        è·å–æ›´é«˜ä¸€çº§çš„tierï¼ˆç”¨äºé™çº§é‡è¯•ï¼‰

        Args:
            current_tier: å½“å‰tier

        Returns:
            æ›´é«˜ä¸€çº§çš„tierï¼Œå¦‚æœå·²ç»æ˜¯æœ€é«˜çº§åˆ™è¿”å›None
        """
        if current_tier == LLMTier.SMALL:
            return LLMTier.MEDIUM
        elif current_tier == LLMTier.MEDIUM:
            return LLMTier.LARGE
        else:
            return None  # Already at highest tier

    def get_quick_llm(self) -> ChatOpenAI:
        """
        è·å–å¿«é€ŸLLMï¼ˆç”¨äºç®€å•ä»»åŠ¡ï¼‰
        ä¿æŒå‘åå…¼å®¹
        """
        if self.enable_small_model_routing:
            return self._create_llm(self.small_model, LLMTier.SMALL)
        else:
            return self._create_llm(self.medium_model, LLMTier.MEDIUM)

    def get_deep_llm(self) -> ChatOpenAI:
        """
        è·å–æ·±åº¦æ€è€ƒLLMï¼ˆç”¨äºå¤æ‚ä»»åŠ¡ï¼‰
        ä¿æŒå‘åå…¼å®¹
        """
        return self._create_llm(self.large_model, LLMTier.LARGE)


# å…¨å±€LLMè·¯ç”±å™¨å®ä¾‹ï¼ˆå•ä¾‹ï¼‰
_global_router: Optional[LLMRouter] = None


def get_llm_router(config: Optional[Dict[str, Any]] = None) -> LLMRouter:
    """
    è·å–å…¨å±€LLMè·¯ç”±å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    Args:
        config: é…ç½®å­—å…¸ï¼ˆä»…åœ¨é¦–æ¬¡è°ƒç”¨æ—¶éœ€è¦ï¼‰

    Returns:
        LLMRouterå®ä¾‹
    """
    global _global_router

    if _global_router is None:
        if config is None:
            raise ValueError("Config required for first-time router initialization")
        _global_router = LLMRouter(config)

    return _global_router


def reset_llm_router():
    """é‡ç½®å…¨å±€è·¯ç”±å™¨ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    global _global_router
    _global_router = None
