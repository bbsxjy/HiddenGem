import chromadb
from chromadb.config import Settings
from openai import OpenAI
import dashscope
from dashscope import TextEmbedding
import os
import threading
import hashlib
from typing import Dict, Optional, List

# å¯¼å…¥è‡ªå®šä¹‰å¼‚å¸¸
from .memory_exceptions import (
    EmbeddingError,
    EmbeddingServiceUnavailable,
    EmbeddingTextTooLong,
    EmbeddingInvalidInput,
    MemoryDisabled
)

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_manager import get_logger
logger = get_logger("agents.utils.memory")


class ChromaDBManager:
    """å•ä¾‹ChromaDBç®¡ç†å™¨ï¼Œé¿å…å¹¶å‘åˆ›å»ºé›†åˆçš„å†²çª

    æ”¯æŒæŒä¹…åŒ–å’ŒéæŒä¹…åŒ–ä¸¤ç§æ¨¡å¼ï¼š
    - éæŒä¹…åŒ–ï¼šç”¨äºTradingAgentså†…éƒ¨çš„çŸ­æœŸè®°å¿†ï¼ˆä¼šè¯çº§ï¼‰
    - æŒä¹…åŒ–ï¼šç”¨äºé•¿æœŸè®°å¿†å­˜å‚¨ï¼ˆæ ¼è¨€åº“ï¼‰
    """

    _instance = None
    _lock = threading.Lock()
    _collections: Dict[str, any] = {}
    _client = None
    _persistent_client = None  # æ–°å¢ï¼šæŒä¹…åŒ–å®¢æˆ·ç«¯

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(ChromaDBManager, cls).__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if not self._initialized:
            try:
                # è‡ªåŠ¨æ£€æµ‹æ“ä½œç³»ç»Ÿç‰ˆæœ¬å¹¶ä½¿ç”¨æœ€ä¼˜é…ç½®
                import platform
                system = platform.system()

                if system == "Windows":
                    # ä½¿ç”¨æ”¹è¿›çš„Windows 11æ£€æµ‹
                    from .chromadb_win11_config import is_windows_11
                    if is_windows_11():
                        # Windows 11 æˆ–æ›´æ–°ç‰ˆæœ¬ï¼Œä½¿ç”¨ä¼˜åŒ–é…ç½®
                        from .chromadb_win11_config import get_win11_chromadb_client
                        self._client = get_win11_chromadb_client()
                        logger.info(f" [ChromaDB] Windows 11ä¼˜åŒ–é…ç½®åˆå§‹åŒ–å®Œæˆ (æ„å»ºå·: {platform.version()})")
                    else:
                        # Windows 10 æˆ–æ›´è€ç‰ˆæœ¬ï¼Œä½¿ç”¨å…¼å®¹é…ç½®
                        from .chromadb_win10_config import get_win10_chromadb_client
                        self._client = get_win10_chromadb_client()
                        logger.info(f" [ChromaDB] Windows 10å…¼å®¹é…ç½®åˆå§‹åŒ–å®Œæˆ")
                else:
                    # éWindowsç³»ç»Ÿï¼Œä½¿ç”¨æ ‡å‡†é…ç½®
                    settings = Settings(
                        allow_reset=True,
                        anonymized_telemetry=False,
                        is_persistent=False
                    )
                    self._client = chromadb.Client(settings)
                    logger.info(f" [ChromaDB] {system}æ ‡å‡†é…ç½®åˆå§‹åŒ–å®Œæˆ")

                # åˆå§‹åŒ–æŒä¹…åŒ–å®¢æˆ·ç«¯ï¼ˆç”¨äºé•¿æœŸè®°å¿†ï¼‰
                persist_path = os.getenv('MEMORY_PERSIST_PATH', './memory_db/maxims')
                try:
                    self._persistent_client = chromadb.PersistentClient(path=persist_path)
                    logger.info(f" [ChromaDB] æŒä¹…åŒ–å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ: {persist_path}")
                except Exception as persist_error:
                    logger.warning(f" [ChromaDB] æŒä¹…åŒ–å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {persist_error}")
                    self._persistent_client = None

                self._initialized = True
            except Exception as e:
                logger.error(f" [ChromaDB] åˆå§‹åŒ–å¤±è´¥: {e}")
                # ä½¿ç”¨æœ€ç®€å•çš„é…ç½®ä½œä¸ºå¤‡ç”¨
                try:
                    settings = Settings(
                        allow_reset=True,
                        anonymized_telemetry=False,  # å…³é”®ï¼šç¦ç”¨é¥æµ‹
                        is_persistent=False
                    )
                    self._client = chromadb.Client(settings)
                    logger.info(f" [ChromaDB] ä½¿ç”¨å¤‡ç”¨é…ç½®åˆå§‹åŒ–å®Œæˆ")
                except Exception as backup_error:
                    # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
                    self._client = chromadb.Client()
                    logger.warning(f" [ChromaDB] ä½¿ç”¨æœ€ç®€é…ç½®åˆå§‹åŒ–: {backup_error}")
                self._initialized = True

    def get_or_create_collection(self, name: str, persistent: bool = False):
        """çº¿ç¨‹å®‰å…¨åœ°è·å–æˆ–åˆ›å»ºé›†åˆ

        Args:
            name: é›†åˆåç§°
            persistent: æ˜¯å¦ä½¿ç”¨æŒä¹…åŒ–å®¢æˆ·ç«¯
                - Falseï¼ˆé»˜è®¤ï¼‰ï¼šä½¿ç”¨å†…å­˜å®¢æˆ·ç«¯ï¼ˆä¼šè¯çº§ï¼‰
                - Trueï¼šä½¿ç”¨æŒä¹…åŒ–å®¢æˆ·ç«¯ï¼ˆæ°¸ä¹…å­˜å‚¨ï¼‰
        """
        with self._lock:
            # ä¸ºæŒä¹…åŒ–é›†åˆä½¿ç”¨ä¸åŒçš„ç¼“å­˜é”®
            cache_key = f"{name}_persistent" if persistent else name

            if cache_key in self._collections:
                logger.info(f" [ChromaDB] ä½¿ç”¨ç¼“å­˜é›†åˆ: {cache_key}")
                return self._collections[cache_key]

            # é€‰æ‹©ä½¿ç”¨å“ªä¸ªå®¢æˆ·ç«¯
            client = self._persistent_client if persistent else self._client

            if persistent and client is None:
                logger.error(f" [ChromaDB] æŒä¹…åŒ–å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå›é€€åˆ°å†…å­˜å®¢æˆ·ç«¯")
                client = self._client
                persistent = False  # å›é€€æ ‡è®°

            try:
                # å°è¯•è·å–ç°æœ‰é›†åˆ
                collection = client.get_collection(name=name)
                logger.info(f" [ChromaDB] è·å–{'æŒä¹…åŒ–' if persistent else 'å†…å­˜'}é›†åˆ: {name}")
            except Exception:
                try:
                    # åˆ›å»ºæ–°é›†åˆ
                    collection = client.create_collection(name=name)
                    logger.info(f" [ChromaDB] åˆ›å»º{'æŒä¹…åŒ–' if persistent else 'å†…å­˜'}é›†åˆ: {name}")
                except Exception as e:
                    # å¯èƒ½æ˜¯å¹¶å‘åˆ›å»ºï¼Œå†æ¬¡å°è¯•è·å–
                    try:
                        collection = client.get_collection(name=name)
                        logger.info(f" [ChromaDB] å¹¶å‘åˆ›å»ºåè·å–é›†åˆ: {name}")
                    except Exception as final_error:
                        logger.error(f" [ChromaDB] é›†åˆæ“ä½œå¤±è´¥: {name}, é”™è¯¯: {final_error}")
                        raise final_error

            # ç¼“å­˜é›†åˆ
            self._collections[cache_key] = collection
            return collection


class FinancialSituationMemory:
    """è´¢åŠ¡æƒ…å†µè®°å¿†åº“

    å­˜å‚¨ç²—ç²’åº¦çš„"ç»éªŒæ ¼è¨€"ï¼Œç”¨äºå¿«é€Ÿæ£€ç´¢ç›¸ä¼¼æƒ…å†µçš„å»ºè®®ã€‚
    æ”¯æŒæŒä¹…åŒ–å’ŒéæŒä¹…åŒ–ä¸¤ç§æ¨¡å¼ã€‚
    """

    def __init__(self, name, config, persistent: bool = False):
        """åˆå§‹åŒ–è®°å¿†åº“

        Args:
            name: é›†åˆåç§°
            config: é…ç½®å­—å…¸
            persistent: æ˜¯å¦æŒä¹…åŒ–
                - Falseï¼ˆé»˜è®¤ï¼‰: ä¼šè¯çº§è®°å¿†ï¼ˆæœåŠ¡å™¨é‡å¯æ¸…ç©ºï¼‰
                - True: æ°¸ä¹…å­˜å‚¨ï¼ˆç”¨äºé•¿æœŸæ ¼è¨€åº“ï¼‰
        """
        self.name = name
        self.persistent = persistent
        self.config = config
        self.llm_provider = config.get("llm_provider", "openai").lower()

        # é…ç½®å‘é‡ç¼“å­˜çš„é•¿åº¦é™åˆ¶ï¼ˆå‘é‡ç¼“å­˜é»˜è®¤å¯ç”¨é•¿åº¦æ£€æŸ¥ï¼‰
        self.max_embedding_length = int(os.getenv('MAX_EMBEDDING_CONTENT_LENGTH', '50000'))  # é»˜è®¤50Kå­—ç¬¦
        self.enable_embedding_length_check = os.getenv('ENABLE_EMBEDDING_LENGTH_CHECK', 'true').lower() == 'true'  # å‘é‡ç¼“å­˜é»˜è®¤å¯ç”¨

        # å®šä¹‰å„æ¨¡å‹çš„æœ€å¤§tokené™åˆ¶ï¼ˆæ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼‰
        self.model_token_limits = {
            'BAAI/bge-large-zh-v1.5': 512,      # SiliconFlow: 512 tokens
            'Qwen/Qwen3-Embedding-8B': 8192,    # SiliconFlow: 8192 tokens (Qwen3 Embedding)
            'text-embedding-v3': 8192,          # DashScope: 8192 tokens
            'text-embedding-3-small': 8191,     # OpenAI: 8191 tokens
            'nomic-embed-text': 8192,           # Ollama: 8192 tokens
        }

        # æ¯ä¸ªtokençº¦ç­‰äº2ä¸ªä¸­æ–‡å­—ç¬¦ï¼ˆæ›´ä¿å®ˆçš„ä¼°è®¡ï¼‰
        # æ³¨æ„ï¼šå¯¹äºSiliconFlowçš„512 tokensé™åˆ¶ï¼Œä½¿ç”¨2ä¼šå¾—åˆ°çº¦460å­—ç¬¦ï¼ˆç•™10%ä½™é‡åï¼‰
        # å¯¹äº8192 tokensçš„æ¨¡å‹ï¼Œä¼šå¾—åˆ°çº¦14700å­—ç¬¦
        self.chars_per_token = 2  # ä»3æ”¹ä¸º2ï¼Œæ›´ä¿å®ˆçš„ä¼°è®¡

        # Embeddingç»“æœç¼“å­˜ï¼šé¿å…çŸ­æ—¶é—´å†…é‡å¤è°ƒç”¨
        self._embedding_cache = {}
        self._embedding_cache_ttl = 300  # ç¼“å­˜5åˆ†é’Ÿ

        # æ ¹æ®LLMæä¾›å•†é€‰æ‹©åµŒå…¥æ¨¡å‹å’Œå®¢æˆ·ç«¯
        # åˆå§‹åŒ–é™çº§é€‰é¡¹æ ‡å¿—
        self.fallback_available = False
        
        if self.llm_provider == "dashscope" or self.llm_provider == "alibaba":
            self.embedding = "text-embedding-v3"
            self.client = None  # DashScopeä¸éœ€è¦OpenAIå®¢æˆ·ç«¯

            # è®¾ç½®DashScope APIå¯†é’¥
            dashscope_key = os.getenv('DASHSCOPE_API_KEY')
            if dashscope_key:
                try:
                    # å°è¯•å¯¼å…¥å’Œåˆå§‹åŒ–DashScope
                    import dashscope
                    from dashscope import TextEmbedding

                    dashscope.api_key = dashscope_key
                    logger.info(f" DashScope APIå¯†é’¥å·²é…ç½®ï¼Œå¯ç”¨è®°å¿†åŠŸèƒ½")

                    # å¯é€‰ï¼šæµ‹è¯•APIè¿æ¥ï¼ˆç®€å•éªŒè¯ï¼‰
                    # è¿™é‡Œä¸åšå®é™…è°ƒç”¨ï¼ŒåªéªŒè¯å¯¼å…¥å’Œå¯†é’¥è®¾ç½®

                except ImportError as e:
                    # DashScopeåŒ…æœªå®‰è£…
                    logger.error(f" DashScopeåŒ…æœªå®‰è£…: {e}")
                    self.client = "DISABLED"
                    logger.warning(f" è®°å¿†åŠŸèƒ½å·²ç¦ç”¨")

                except Exception as e:
                    # å…¶ä»–åˆå§‹åŒ–é”™è¯¯
                    logger.error(f" DashScopeåˆå§‹åŒ–å¤±è´¥: {e}")
                    self.client = "DISABLED"
                    logger.warning(f" è®°å¿†åŠŸèƒ½å·²ç¦ç”¨")
            else:
                # æ²¡æœ‰DashScopeå¯†é’¥ï¼Œç¦ç”¨è®°å¿†åŠŸèƒ½
                self.client = "DISABLED"
                logger.warning(f" æœªæ‰¾åˆ°DASHSCOPE_API_KEYï¼Œè®°å¿†åŠŸèƒ½å·²ç¦ç”¨")
                logger.info(f" ç³»ç»Ÿå°†ç»§ç»­è¿è¡Œï¼Œä½†ä¸ä¼šä¿å­˜æˆ–æ£€ç´¢å†å²è®°å¿†")
        elif self.llm_provider == "siliconflow":
            # SiliconFlowä½¿ç”¨OpenAIå…¼å®¹API
            siliconflow_key = os.getenv('SILICONFLOW_API_KEY')
            if siliconflow_key:
                try:
                    #  è¯»å–ç”¨æˆ·é…ç½®çš„embeddingæ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
                    custom_embedding_model = os.getenv('EMBEDDING_MODEL')
                    if custom_embedding_model:
                        self.embedding = custom_embedding_model
                        logger.info(f" ä½¿ç”¨è‡ªå®šä¹‰embeddingæ¨¡å‹: {self.embedding}")
                    else:
                        # é»˜è®¤ä½¿ç”¨ BAAI/bge-large-zh-v1.5
                        self.embedding = "BAAI/bge-large-zh-v1.5"
                        logger.info(f" ä½¿ç”¨é»˜è®¤embeddingæ¨¡å‹: {self.embedding}")

                    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨tokené™åˆ¶é…ç½®ä¸­
                    if self.embedding not in self.model_token_limits:
                        logger.warning(f" æœªçŸ¥çš„embeddingæ¨¡å‹: {self.embedding}ï¼Œå°†ä½¿ç”¨é»˜è®¤tokené™åˆ¶8192")
                        self.model_token_limits[self.embedding] = 8192

                    self.client = OpenAI(
                        api_key=siliconflow_key,
                        base_url="https://api.siliconflow.cn/v1",
                        max_retries=0  # ç¦ç”¨é‡è¯•ï¼Œåªå°è¯•ä¸€æ¬¡
                    )
                    logger.info(f" SiliconFlow embeddingå·²é…ç½®ï¼Œæ¨¡å‹: {self.embedding}, "
                               f"tokené™åˆ¶: {self.model_token_limits[self.embedding]}")
                except Exception as e:
                    logger.error(f" SiliconFlow embeddingåˆå§‹åŒ–å¤±è´¥: {e}")
                    self.client = "DISABLED"
                    logger.warning(f" SiliconFlowè®°å¿†åŠŸèƒ½å·²ç¦ç”¨")
            else:
                # æ²¡æœ‰SiliconFlowå¯†é’¥ï¼Œç¦ç”¨è®°å¿†åŠŸèƒ½
                self.client = "DISABLED"
                logger.warning(f" æœªæ‰¾åˆ°SILICONFLOW_API_KEYï¼Œè®°å¿†åŠŸèƒ½å·²ç¦ç”¨")
                logger.info(f" ç³»ç»Ÿå°†ç»§ç»­è¿è¡Œï¼Œä½†ä¸ä¼šä¿å­˜æˆ–æ£€ç´¢å†å²è®°å¿†")
        elif self.llm_provider == "qianfan":
            # åƒå¸†ï¼ˆæ–‡å¿ƒä¸€è¨€ï¼‰embeddingé…ç½®
            # åƒå¸†ç›®å‰æ²¡æœ‰ç‹¬ç«‹çš„embedding APIï¼Œä½¿ç”¨é˜¿é‡Œç™¾ç‚¼ä½œä¸ºé™çº§é€‰é¡¹
            dashscope_key = os.getenv('DASHSCOPE_API_KEY')
            if dashscope_key:
                try:
                    # ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥æœåŠ¡ä½œä¸ºåƒå¸†çš„embeddingè§£å†³æ–¹æ¡ˆ
                    import dashscope
                    from dashscope import TextEmbedding

                    dashscope.api_key = dashscope_key
                    self.embedding = "text-embedding-v3"
                    self.client = None
                    logger.info(f" åƒå¸†ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥æœåŠ¡")
                except ImportError as e:
                    logger.error(f" DashScopeåŒ…æœªå®‰è£…: {e}")
                    self.client = "DISABLED"
                    logger.warning(f" åƒå¸†è®°å¿†åŠŸèƒ½å·²ç¦ç”¨")
                except Exception as e:
                    logger.error(f" åƒå¸†åµŒå…¥åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.client = "DISABLED"
                    logger.warning(f" åƒå¸†è®°å¿†åŠŸèƒ½å·²ç¦ç”¨")
            else:
                # æ²¡æœ‰DashScopeå¯†é’¥ï¼Œç¦ç”¨è®°å¿†åŠŸèƒ½
                self.client = "DISABLED"
                logger.warning(f" åƒå¸†æœªæ‰¾åˆ°DASHSCOPE_API_KEYï¼Œè®°å¿†åŠŸèƒ½å·²ç¦ç”¨")
                logger.info(f" ç³»ç»Ÿå°†ç»§ç»­è¿è¡Œï¼Œä½†ä¸ä¼šä¿å­˜æˆ–æ£€ç´¢å†å²è®°å¿†")
        elif self.llm_provider == "deepseek":
            # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨OpenAIåµŒå…¥
            force_openai = os.getenv('FORCE_OPENAI_EMBEDDING', 'false').lower() == 'true'

            if not force_openai:
                # å°è¯•ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥
                dashscope_key = os.getenv('DASHSCOPE_API_KEY')
                if dashscope_key:
                    try:
                        # æµ‹è¯•é˜¿é‡Œç™¾ç‚¼æ˜¯å¦å¯ç”¨
                        import dashscope
                        from dashscope import TextEmbedding

                        dashscope.api_key = dashscope_key
                        # éªŒè¯TextEmbeddingå¯ç”¨æ€§ï¼ˆä¸éœ€è¦å®é™…è°ƒç”¨ï¼‰
                        self.embedding = "text-embedding-v3"
                        self.client = None
                        logger.info(f" DeepSeekä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥æœåŠ¡")
                    except ImportError as e:
                        logger.error(f" DashScopeåŒ…æœªå®‰è£…: {e}")
                        dashscope_key = None  # å¼ºåˆ¶é™çº§
                    except Exception as e:
                        logger.error(f" é˜¿é‡Œç™¾ç‚¼åµŒå…¥åˆå§‹åŒ–å¤±è´¥: {e}")
                        dashscope_key = None  # å¼ºåˆ¶é™çº§
            else:
                dashscope_key = None  # è·³è¿‡é˜¿é‡Œç™¾ç‚¼

            if not dashscope_key or force_openai:
                # é™çº§åˆ°OpenAIåµŒå…¥
                self.embedding = "text-embedding-3-small"
                openai_key = os.getenv('OPENAI_API_KEY')
                if openai_key:
                    self.client = OpenAI(
                        api_key=openai_key,
                        base_url=config.get("backend_url", "https://api.openai.com/v1"),
                        max_retries=0  # ç¦ç”¨é‡è¯•ï¼Œåªå°è¯•ä¸€æ¬¡
                    )
                    logger.warning(f" DeepSeekå›é€€åˆ°OpenAIåµŒå…¥æœåŠ¡")
                else:
                    # æœ€åå°è¯•DeepSeekè‡ªå·±çš„åµŒå…¥
                    deepseek_key = os.getenv('DEEPSEEK_API_KEY')
                    if deepseek_key:
                        try:
                            self.client = OpenAI(
                                api_key=deepseek_key,
                                base_url="https://api.deepseek.com",
                                max_retries=0  # ç¦ç”¨é‡è¯•ï¼Œåªå°è¯•ä¸€æ¬¡
                            )
                            logger.info(f" DeepSeekä½¿ç”¨è‡ªå·±çš„åµŒå…¥æœåŠ¡")
                        except Exception as e:
                            logger.error(f" DeepSeekåµŒå…¥æœåŠ¡ä¸å¯ç”¨: {e}")
                            # ç¦ç”¨å†…å­˜åŠŸèƒ½
                            self.client = "DISABLED"
                            logger.info(f" å†…å­˜åŠŸèƒ½å·²ç¦ç”¨ï¼Œç³»ç»Ÿå°†ç»§ç»­è¿è¡Œä½†ä¸ä¿å­˜å†å²è®°å¿†")
                    else:
                        # ç¦ç”¨å†…å­˜åŠŸèƒ½è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                        self.client = "DISABLED"
                        logger.info(f" æœªæ‰¾åˆ°å¯ç”¨çš„åµŒå…¥æœåŠ¡ï¼Œå†…å­˜åŠŸèƒ½å·²ç¦ç”¨")
        elif self.llm_provider == "google":
            # Google AIä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå¦åˆ™ç¦ç”¨è®°å¿†åŠŸèƒ½
            dashscope_key = os.getenv('DASHSCOPE_API_KEY')
            openai_key = os.getenv('OPENAI_API_KEY')
            
            if dashscope_key:
                try:
                    # å°è¯•åˆå§‹åŒ–DashScope
                    import dashscope
                    from dashscope import TextEmbedding

                    self.embedding = "text-embedding-v3"
                    self.client = None
                    dashscope.api_key = dashscope_key
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰OpenAIå¯†é’¥ä½œä¸ºé™çº§é€‰é¡¹
                    if openai_key:
                        logger.info(f" Google AIä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥æœåŠ¡ï¼ˆOpenAIä½œä¸ºé™çº§é€‰é¡¹ï¼‰")
                        self.fallback_available = True
                        self.fallback_client = OpenAI(
                            api_key=openai_key,
                            base_url=config["backend_url"],
                            max_retries=0  # ç¦ç”¨é‡è¯•ï¼Œåªå°è¯•ä¸€æ¬¡
                        )
                        self.fallback_embedding = "text-embedding-3-small"
                    else:
                        logger.info(f" Google AIä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥æœåŠ¡ï¼ˆæ— é™çº§é€‰é¡¹ï¼‰")
                        self.fallback_available = False
                        
                except ImportError as e:
                    logger.error(f" DashScopeåŒ…æœªå®‰è£…: {e}")
                    self.client = "DISABLED"
                    logger.warning(f" Google AIè®°å¿†åŠŸèƒ½å·²ç¦ç”¨")
                except Exception as e:
                    logger.error(f" DashScopeåˆå§‹åŒ–å¤±è´¥: {e}")
                    self.client = "DISABLED"
                    logger.warning(f" Google AIè®°å¿†åŠŸèƒ½å·²ç¦ç”¨")
            else:
                # æ²¡æœ‰DashScopeå¯†é’¥ï¼Œç¦ç”¨è®°å¿†åŠŸèƒ½
                self.client = "DISABLED"
                self.fallback_available = False
                logger.warning(f" Google AIæœªæ‰¾åˆ°DASHSCOPE_API_KEYï¼Œè®°å¿†åŠŸèƒ½å·²ç¦ç”¨")
                logger.info(f" ç³»ç»Ÿå°†ç»§ç»­è¿è¡Œï¼Œä½†ä¸ä¼šä¿å­˜æˆ–æ£€ç´¢å†å²è®°å¿†")
        elif self.llm_provider == "openrouter":
            # OpenRouteræ”¯æŒï¼šä¼˜å…ˆä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥ï¼Œå¦åˆ™ç¦ç”¨è®°å¿†åŠŸèƒ½
            dashscope_key = os.getenv('DASHSCOPE_API_KEY')
            if dashscope_key:
                try:
                    # å°è¯•ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥
                    import dashscope
                    from dashscope import TextEmbedding

                    self.embedding = "text-embedding-v3"
                    self.client = None
                    dashscope.api_key = dashscope_key
                    logger.info(f" OpenRouterä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥æœåŠ¡")
                except ImportError as e:
                    logger.error(f" DashScopeåŒ…æœªå®‰è£…: {e}")
                    self.client = "DISABLED"
                    logger.warning(f" OpenRouterè®°å¿†åŠŸèƒ½å·²ç¦ç”¨")
                except Exception as e:
                    logger.error(f" DashScopeåˆå§‹åŒ–å¤±è´¥: {e}")
                    self.client = "DISABLED"
                    logger.warning(f" OpenRouterè®°å¿†åŠŸèƒ½å·²ç¦ç”¨")
            else:
                # æ²¡æœ‰DashScopeå¯†é’¥ï¼Œç¦ç”¨è®°å¿†åŠŸèƒ½
                self.client = "DISABLED"
                logger.warning(f" OpenRouteræœªæ‰¾åˆ°DASHSCOPE_API_KEYï¼Œè®°å¿†åŠŸèƒ½å·²ç¦ç”¨")
                logger.info(f" ç³»ç»Ÿå°†ç»§ç»­è¿è¡Œï¼Œä½†ä¸ä¼šä¿å­˜æˆ–æ£€ç´¢å†å²è®°å¿†")
        elif config["backend_url"] == "http://localhost:11434/v1":
            self.embedding = "nomic-embed-text"
            self.client = OpenAI(
                base_url=config["backend_url"],
                max_retries=0  # ç¦ç”¨é‡è¯•ï¼Œåªå°è¯•ä¸€æ¬¡
            )
        else:
            self.embedding = "text-embedding-3-small"
            openai_key = os.getenv('OPENAI_API_KEY')
            if openai_key:
                self.client = OpenAI(
                    api_key=openai_key,
                    base_url=config["backend_url"],
                    max_retries=0  # ç¦ç”¨é‡è¯•ï¼Œåªå°è¯•ä¸€æ¬¡
                )
            else:
                self.client = "DISABLED"
                logger.warning(f" æœªæ‰¾åˆ°OPENAI_API_KEYï¼Œè®°å¿†åŠŸèƒ½å·²ç¦ç”¨")

        # ä½¿ç”¨å•ä¾‹ChromaDBç®¡ç†å™¨
        self.chroma_manager = ChromaDBManager()
        self.situation_collection = self.chroma_manager.get_or_create_collection(
            name=name,
            persistent=persistent
        )

        logger.info(f" [FinancialSituationMemory] åˆå§‹åŒ–å®Œæˆ: {name} "
                   f"({'æŒä¹…åŒ–' if persistent else 'ä¼šè¯çº§'}è®°å¿†)")

    def _get_model_max_chars(self):
        """è·å–å½“å‰æ¨¡å‹çš„æœ€å¤§å­—ç¬¦æ•°é™åˆ¶"""
        max_tokens = self.model_token_limits.get(self.embedding, 8192)  # é»˜è®¤8192 tokens
        max_chars = max_tokens * self.chars_per_token  # è½¬æ¢ä¸ºå­—ç¬¦æ•°

        # ç•™10%ä½™é‡ï¼Œé¿å…è¾¹ç•Œæƒ…å†µ
        max_chars = int(max_chars * 0.9)

        logger.debug(f" æ¨¡å‹{self.embedding}æœ€å¤§é•¿åº¦: {max_tokens} tokens â‰ˆ {max_chars} å­—ç¬¦")
        return max_chars

    def _smart_text_truncation(self, text, max_length=None):
        """
        æ™ºèƒ½æ–‡æœ¬æˆªæ–­ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§

        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_length: æœ€å¤§é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ¨¡å‹é™åˆ¶

        Returns:
            (truncated_text, was_truncated)
        """
        if max_length is None:
            max_length = self._get_model_max_chars()

        if len(text) <= max_length:
            return text, False  # è¿”å›åŸæ–‡æœ¬å’Œæ˜¯å¦æˆªæ–­çš„æ ‡å¿—

        logger.info(f" æ–‡æœ¬éœ€è¦æˆªæ–­ï¼š{len(text)} å­—ç¬¦ â†’ {max_length} å­—ç¬¦")

        # ç­–ç•¥1ï¼šå°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
        sentences = text.split('ã€‚')
        if len(sentences) > 1:
            truncated = ""
            for sentence in sentences:
                if len(truncated + sentence + 'ã€‚') <= max_length - 50:  # ç•™50å­—ç¬¦ä½™é‡
                    truncated += sentence + 'ã€‚'
                else:
                    break
            if len(truncated) > max_length // 2:  # è‡³å°‘ä¿ç•™ä¸€åŠå†…å®¹
                logger.info(f" åœ¨å¥å­è¾¹ç•Œæˆªæ–­ï¼Œä¿ç•™{len(truncated)}/{len(text)}å­—ç¬¦")
                return truncated, True

        # ç­–ç•¥2ï¼šå°è¯•åœ¨æ®µè½è¾¹ç•Œæˆªæ–­
        paragraphs = text.split('\n')
        if len(paragraphs) > 1:
            truncated = ""
            for paragraph in paragraphs:
                if len(truncated + paragraph + '\n') <= max_length - 50:
                    truncated += paragraph + '\n'
                else:
                    break
            if len(truncated) > max_length // 2:
                logger.info(f" åœ¨æ®µè½è¾¹ç•Œæˆªæ–­ï¼Œä¿ç•™{len(truncated)}/{len(text)}å­—ç¬¦")
                return truncated, True

        # ç­–ç•¥3ï¼šä¿ç•™å‰éƒ¨åˆ†ï¼ˆæ›´é‡è¦ï¼‰
        # é€šå¸¸åˆ†ææŠ¥å‘Šçš„å‰é¢éƒ¨åˆ†åŒ…å«æ ¸å¿ƒç»“è®º
        truncated = text[:max_length - 50] + "\n...[å†…å®¹å·²æˆªæ–­]"
        logger.warning(f" å¼ºåˆ¶æˆªæ–­å‰{max_length}å­—ç¬¦ï¼ŒåŸé•¿åº¦{len(text)}å­—ç¬¦")
        return truncated, True

    def _chunk_and_embed(self, text):
        """
        ğŸ†• å°†è¶…é•¿æ–‡æœ¬åˆ†å—å¹¶ç”Ÿæˆembeddingï¼ˆå¹³å‡åˆå¹¶ç­–ç•¥ï¼‰

        Args:
            text: è¶…é•¿æ–‡æœ¬

        Returns:
            embeddingå‘é‡ï¼ˆå¤šä¸ªchunkçš„å¹³å‡å‘é‡ï¼‰
        """
        import numpy as np

        chunk_size = self.max_embedding_length - 100  # ç•™100å­—ç¬¦ä½™é‡
        overlap = chunk_size // 4  # 25% é‡å ä»¥ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§

        # åˆ†å—
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]

            # å°è¯•åœ¨å¥å­æˆ–æ®µè½è¾¹ç•Œåˆ†å‰²
            if end < len(text):
                # æŸ¥æ‰¾æœ€è¿‘çš„å¥å·æˆ–æ¢è¡Œ
                last_period = chunk.rfind('ã€‚')
                last_newline = chunk.rfind('\n')
                split_point = max(last_period, last_newline)

                if split_point > chunk_size // 2:  # è‡³å°‘ä¿ç•™ä¸€åŠå†…å®¹
                    chunk = chunk[:split_point + 1]
                    end = start + len(chunk)

            chunks.append(chunk)
            start = end - overlap  # é‡å éƒ¨åˆ†

        logger.info(f"ğŸ“¦ æ–‡æœ¬åˆ†å—: {len(text)}å­—ç¬¦ â†’ {len(chunks)}ä¸ªå—ï¼ˆæ¯å—~{chunk_size}å­—ç¬¦ï¼Œé‡å {overlap}å­—ç¬¦ï¼‰")

        # ä¸ºæ¯ä¸ªchunkç”Ÿæˆembedding
        chunk_embeddings = []
        for i, chunk in enumerate(chunks):
            logger.debug(f"  å¤„ç†ç¬¬{i+1}/{len(chunks)}å—...")

            # é€’å½’è°ƒç”¨get_embeddingï¼ˆä½†è¿™æ¬¡chunkä¸ä¼šè¶…è¿‡é™åˆ¶ï¼‰
            # æš‚æ—¶ç¦ç”¨é•¿åº¦æ£€æŸ¥ä»¥é¿å…æ— é™é€’å½’
            original_check = self.enable_embedding_length_check
            self.enable_embedding_length_check = False

            try:
                processed_chunk, _ = self._smart_text_truncation(chunk, self.max_embedding_length)
                embedding = self._generate_embedding_direct(processed_chunk)
                chunk_embeddings.append(embedding)
            finally:
                self.enable_embedding_length_check = original_check

        # åˆå¹¶æ‰€æœ‰chunkçš„embeddingï¼ˆç®€å•å¹³å‡ï¼‰
        avg_embedding = np.mean(chunk_embeddings, axis=0)

        logger.info(f"âœ… åˆ†å—å¤„ç†å®Œæˆ: {len(chunks)}ä¸ªå— â†’ å¹³å‡embeddingï¼ˆç»´åº¦{len(avg_embedding)}ï¼‰")

        # å­˜å‚¨æ–‡æœ¬å¤„ç†ä¿¡æ¯
        self._last_text_info = {
            'original_length': len(text),
            'processed_length': sum(len(c) for c in chunks),
            'was_truncated': False,
            'was_chunked': True,
            'num_chunks': len(chunks),
            'chunk_size': chunk_size,
            'overlap': overlap,
            'provider': self.llm_provider,
            'strategy': 'chunking_with_overlap'
        }

        return avg_embedding.tolist()

    def _generate_embedding_direct(self, text):
        """
        ğŸ†• ç›´æ¥ç”Ÿæˆembeddingï¼ˆä¸ç»è¿‡é•¿åº¦æ£€æŸ¥å’Œç¼“å­˜ï¼‰

        å†…éƒ¨æ–¹æ³•ï¼Œä»…ä¾›_chunk_and_embedä½¿ç”¨
        """
        try:
            # æ ¹æ®provideré€‰æ‹©å®ç°
            if self.llm_provider == "openai":
                return self.client.embeddings.create(
                    input=text,
                    model="text-embedding-3-large"
                ).data[0].embedding
            elif self.llm_provider == "dashscope":
                # Dashscope embeddingå®ç°
                from http import HTTPStatus
                import dashscope

                dashscope.api_key = self.llm_api_key

                resp = dashscope.TextEmbedding.call(
                    model=dashscope.TextEmbedding.Models.text_embedding_v3,
                    input=text
                )

                if resp.status_code == HTTPStatus.OK:
                    return resp.output['embeddings'][0]['embedding']
                else:
                    raise EmbeddingServiceUnavailable(f"Dashscopeé”™è¯¯: {resp.message}")
            else:
                raise EmbeddingServiceUnavailable(f"ä¸æ”¯æŒçš„provider: {self.llm_provider}")

        except Exception as e:
            logger.error(f"ç”Ÿæˆembeddingå¤±è´¥: {e}")
            raise EmbeddingServiceUnavailable(str(e))


    def get_embedding(self, text):
        """Get embedding for a text using the configured provider
        æ·»åŠ ç¼“å­˜æœºåˆ¶ï¼Œé¿å…çŸ­æ—¶é—´å†…é‡å¤è°ƒç”¨

        Raises:
            MemoryDisabled: å½“MemoryåŠŸèƒ½è¢«ç¦ç”¨æ—¶
            EmbeddingInvalidInput: å½“è¾“å…¥æ–‡æœ¬æ— æ•ˆæ—¶
            EmbeddingTextTooLong: å½“æ–‡æœ¬è¶…è¿‡é•¿åº¦é™åˆ¶æ—¶
            EmbeddingServiceUnavailable: å½“embeddingæœåŠ¡ä¸å¯ç”¨æ—¶
        """
        import hashlib
        import time

        # æ£€æŸ¥è®°å¿†åŠŸèƒ½æ˜¯å¦è¢«ç¦ç”¨
        if self.client == "DISABLED":
            logger.error("è®°å¿†åŠŸèƒ½å·²ç¦ç”¨ï¼Œæ— æ³•ç”Ÿæˆembedding")
            raise MemoryDisabled()

        # éªŒè¯è¾“å…¥æ–‡æœ¬
        if not text or not isinstance(text, str):
            reason = "æ–‡æœ¬ä¸ºNone" if not text else f"æ–‡æœ¬ç±»å‹é”™è¯¯: {type(text)}"
            logger.error(f"è¾“å…¥æ–‡æœ¬æ— æ•ˆ: {reason}")
            raise EmbeddingInvalidInput(reason)

        text_length = len(text)
        if text_length == 0:
            logger.error("è¾“å…¥æ–‡æœ¬é•¿åº¦ä¸º0")
            raise EmbeddingInvalidInput("æ–‡æœ¬é•¿åº¦ä¸º0")

        # æ£€æŸ¥ç¼“å­˜
        cache_key = hashlib.md5(text.encode('utf-8')).hexdigest()
        current_time = time.time()

        if cache_key in self._embedding_cache:
            cached_embedding, cached_time = self._embedding_cache[cache_key]
            if current_time - cached_time < self._embedding_cache_ttl:
                logger.debug(f"[Embeddingç¼“å­˜] ä½¿ç”¨ç¼“å­˜å‘é‡ï¼Œç¼“å­˜æ—¶é—´: {int(current_time - cached_time)}ç§’å‰")
                return cached_embedding

        logger.debug(f"[Embedding] ç”Ÿæˆæ–°å‘é‡ï¼Œæ–‡æœ¬é•¿åº¦: {text_length}å­—ç¬¦")

        # ğŸ†• æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å—å¤„ç†ï¼ˆè¶…è¿‡æœ€å¤§é•¿åº¦ï¼‰
        if self.enable_embedding_length_check and text_length > self.max_embedding_length:
            logger.warning(f"æ–‡æœ¬è¿‡é•¿({text_length:,}å­—ç¬¦ > {self.max_embedding_length:,}å­—ç¬¦)ï¼Œå¯ç”¨è‡ªåŠ¨åˆ†å—å¤„ç†")
            return self._chunk_and_embed(text)

        #  æ–°å¢ï¼šæ™ºèƒ½æˆªæ–­æ–‡æœ¬ï¼ˆæ ¹æ®æ¨¡å‹é™åˆ¶ï¼‰
        processed_text, was_truncated = self._smart_text_truncation(text)

        # è®°å½•æ–‡æœ¬ä¿¡æ¯
        if text_length > 1500 or was_truncated:
            logger.info(f" å¤„ç†æ–‡æœ¬: åŸå§‹{text_length}å­—ç¬¦ â†’ å¤„ç†å{len(processed_text)}å­—ç¬¦ "
                       f"({'å·²æˆªæ–­' if was_truncated else 'æœªæˆªæ–­'}), æä¾›å•†: {self.llm_provider}")

        # å­˜å‚¨æ–‡æœ¬å¤„ç†ä¿¡æ¯
        self._last_text_info = {
            'original_length': text_length,
            'processed_length': len(processed_text),
            'was_truncated': was_truncated,
            'was_skipped': False,
            'provider': self.llm_provider,
            'embedding_model': self.embedding,
            'strategy': 'smart_truncation' if was_truncated else 'no_truncation'
        }

        if (self.llm_provider == "dashscope" or
            self.llm_provider == "alibaba" or
            self.llm_provider == "qianfan" or
            (self.llm_provider == "google" and self.client is None) or
            (self.llm_provider == "deepseek" and self.client is None) or
            (self.llm_provider == "openrouter" and self.client is None)):
            # ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼çš„åµŒå…¥æ¨¡å‹
            try:
                # å¯¼å…¥DashScopeæ¨¡å—
                import dashscope
                from dashscope import TextEmbedding

                # æ£€æŸ¥DashScope APIå¯†é’¥æ˜¯å¦å¯ç”¨
                if not hasattr(dashscope, 'api_key') or not dashscope.api_key:
                    logger.warning(f" DashScope APIå¯†é’¥æœªè®¾ç½®ï¼Œè®°å¿†åŠŸèƒ½é™çº§")
                    return [0.0] * 1024  # è¿”å›ç©ºå‘é‡

                # å°è¯•è°ƒç”¨DashScope API
                response = TextEmbedding.call(
                    model=self.embedding,
                    input=processed_text  # ä½¿ç”¨æˆªæ–­åçš„æ–‡æœ¬
                )

                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 200:
                    # æˆåŠŸè·å–embedding
                    embedding = response.output['embeddings'][0]['embedding']
                    logger.debug(f" DashScope embeddingæˆåŠŸï¼Œç»´åº¦: {len(embedding)}")

                    # ç¼“å­˜ç»“æœ
                    self._embedding_cache[cache_key] = (embedding, current_time)
                    logger.debug(f" [Embeddingç¼“å­˜] ç¼“å­˜å‘é‡ï¼ŒTTL: {self._embedding_cache_ttl}ç§’")

                    return embedding
                else:
                    # APIè¿”å›é”™è¯¯çŠ¶æ€ç 
                    error_msg = f"{response.code} - {response.message}"
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºé•¿åº¦é™åˆ¶é”™è¯¯
                    if any(keyword in error_msg.lower() for keyword in ['length', 'token', 'limit', 'exceed']):
                        logger.warning(f" DashScopeé•¿åº¦é™åˆ¶: {error_msg}")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰é™çº§é€‰é¡¹
                        if hasattr(self, 'fallback_available') and self.fallback_available:
                            logger.info(f" å°è¯•ä½¿ç”¨OpenAIé™çº§å¤„ç†é•¿æ–‡æœ¬")
                            try:
                                response = self.fallback_client.embeddings.create(
                                    model=self.fallback_embedding,
                                    input=processed_text  # ä½¿ç”¨æˆªæ–­åçš„æ–‡æœ¬
                                )
                                embedding = response.data[0].embedding
                                logger.info(f" OpenAIé™çº§æˆåŠŸï¼Œç»´åº¦: {len(embedding)}")

                                # ç¼“å­˜ç»“æœ
                                self._embedding_cache[cache_key] = (embedding, current_time)
                                logger.debug(f" [Embeddingç¼“å­˜] ç¼“å­˜å‘é‡ï¼ˆé™çº§ï¼‰ï¼ŒTTL: {self._embedding_cache_ttl}ç§’")

                                return embedding
                            except Exception as fallback_error:
                                logger.error(f" OpenAIé™çº§å¤±è´¥: {str(fallback_error)}")
                                logger.info(f" æ‰€æœ‰é™çº§é€‰é¡¹å¤±è´¥ï¼Œè®°å¿†åŠŸèƒ½é™çº§")
                                return [0.0] * 1024
                        else:
                            logger.info(f" æ— å¯ç”¨é™çº§é€‰é¡¹ï¼Œè®°å¿†åŠŸèƒ½é™çº§")
                            return [0.0] * 1024
                    else:
                        logger.error(f" DashScope APIé”™è¯¯: {error_msg}")
                        return [0.0] * 1024  # è¿”å›ç©ºå‘é‡è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸

            except Exception as e:
                error_str = str(e).lower()
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºé•¿åº¦é™åˆ¶é”™è¯¯
                if any(keyword in error_str for keyword in ['length', 'token', 'limit', 'exceed', 'too long']):
                    logger.warning(f" DashScopeé•¿åº¦é™åˆ¶å¼‚å¸¸: {str(e)}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é™çº§é€‰é¡¹
                    if hasattr(self, 'fallback_available') and self.fallback_available:
                        logger.info(f" å°è¯•ä½¿ç”¨OpenAIé™çº§å¤„ç†é•¿æ–‡æœ¬")
                        try:
                            response = self.fallback_client.embeddings.create(
                                model=self.fallback_embedding,
                                input=processed_text  # ä½¿ç”¨æˆªæ–­åçš„æ–‡æœ¬
                            )
                            embedding = response.data[0].embedding
                            logger.info(f" OpenAIé™çº§æˆåŠŸï¼Œç»´åº¦: {len(embedding)}")

                            # ç¼“å­˜ç»“æœ
                            self._embedding_cache[cache_key] = (embedding, current_time)
                            logger.debug(f" [Embeddingç¼“å­˜] ç¼“å­˜å‘é‡ï¼ˆé™çº§ï¼‰ï¼ŒTTL: {self._embedding_cache_ttl}ç§’")

                            return embedding
                        except Exception as fallback_error:
                            logger.error(f" OpenAIé™çº§å¤±è´¥: {str(fallback_error)}")
                            logger.info(f" æ‰€æœ‰é™çº§é€‰é¡¹å¤±è´¥ï¼Œè®°å¿†åŠŸèƒ½é™çº§")
                            return [0.0] * 1024
                    else:
                        logger.info(f" æ— å¯ç”¨é™çº§é€‰é¡¹ï¼Œè®°å¿†åŠŸèƒ½é™çº§")
                        return [0.0] * 1024
                elif 'import' in error_str:
                    logger.error(f" DashScopeåŒ…æœªå®‰è£…: {str(e)}")
                elif 'connection' in error_str:
                    logger.error(f" DashScopeç½‘ç»œè¿æ¥é”™è¯¯: {str(e)}")
                elif 'timeout' in error_str:
                    logger.error(f" DashScopeè¯·æ±‚è¶…æ—¶: {str(e)}")
                else:
                    logger.error(f" DashScope embeddingå¼‚å¸¸: {str(e)}")
                
                logger.warning(f" è®°å¿†åŠŸèƒ½é™çº§ï¼Œè¿”å›ç©ºå‘é‡")
                return [0.0] * 1024
        else:
            # ä½¿ç”¨OpenAIå…¼å®¹çš„åµŒå…¥æ¨¡å‹
            if self.client is None:
                logger.warning(f" åµŒå…¥å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºå‘é‡")
                return [0.0] * 1024  # è¿”å›ç©ºå‘é‡
            elif self.client == "DISABLED":
                # å†…å­˜åŠŸèƒ½å·²ç¦ç”¨ï¼Œè¿”å›ç©ºå‘é‡
                logger.debug(f" å†…å­˜åŠŸèƒ½å·²ç¦ç”¨ï¼Œè¿”å›ç©ºå‘é‡")
                return [0.0] * 1024  # è¿”å›1024ç»´çš„é›¶å‘é‡

            # å°è¯•è°ƒç”¨OpenAIå…¼å®¹çš„embedding API
            try:
                response = self.client.embeddings.create(
                    model=self.embedding,
                    input=processed_text  # ä½¿ç”¨æˆªæ–­åçš„æ–‡æœ¬
                )
                embedding = response.data[0].embedding
                logger.debug(f" {self.llm_provider} embeddingæˆåŠŸï¼Œç»´åº¦: {len(embedding)}")

                # ç¼“å­˜ç»“æœ
                self._embedding_cache[cache_key] = (embedding, current_time)
                logger.debug(f" [Embeddingç¼“å­˜] ç¼“å­˜å‘é‡ï¼ŒTTL: {self._embedding_cache_ttl}ç§’")

                return embedding

            except Exception as e:
                error_str = str(e).lower()
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºé•¿åº¦é™åˆ¶é”™è¯¯
                length_error_keywords = [
                    'token', 'length', 'too long', 'exceed', 'maximum', 'limit',
                    'context', 'input too large', 'request too large'
                ]
                
                is_length_error = any(keyword in error_str for keyword in length_error_keywords)
                
                if is_length_error:
                    # é•¿åº¦é™åˆ¶é”™è¯¯ï¼šç›´æ¥é™çº§ï¼Œä¸æˆªæ–­é‡è¯•
                    logger.warning(f" {self.llm_provider}é•¿åº¦é™åˆ¶: {str(e)}")
                    logger.info(f" ä¸ºä¿è¯åˆ†æå‡†ç¡®æ€§ï¼Œä¸æˆªæ–­æ–‡æœ¬ï¼Œè®°å¿†åŠŸèƒ½é™çº§")
                else:
                    # å…¶ä»–ç±»å‹çš„é”™è¯¯
                    if 'attributeerror' in error_str:
                        logger.error(f" {self.llm_provider} APIè°ƒç”¨é”™è¯¯: {str(e)}")
                    elif 'connectionerror' in error_str or 'connection' in error_str:
                        logger.error(f" {self.llm_provider}ç½‘ç»œè¿æ¥é”™è¯¯: {str(e)}")
                    elif 'timeout' in error_str:
                        logger.error(f" {self.llm_provider}è¯·æ±‚è¶…æ—¶: {str(e)}")
                    elif 'keyerror' in error_str:
                        logger.error(f" {self.llm_provider}å“åº”æ ¼å¼é”™è¯¯: {str(e)}")
                    else:
                        logger.error(f" {self.llm_provider} embeddingå¼‚å¸¸: {str(e)}")
                
                logger.warning(f" è®°å¿†åŠŸèƒ½é™çº§ï¼Œè¿”å›ç©ºå‘é‡")
                return [0.0] * 1024

    def get_embedding_config_status(self):
        """è·å–å‘é‡ç¼“å­˜é…ç½®çŠ¶æ€"""
        return {
            'enabled': self.enable_embedding_length_check,
            'max_embedding_length': self.max_embedding_length,
            'max_embedding_length_formatted': f"{self.max_embedding_length:,}å­—ç¬¦",
            'provider': self.llm_provider,
            'client_status': 'DISABLED' if self.client == "DISABLED" else 'ENABLED'
        }

    def get_last_text_info(self):
        """è·å–æœ€åå¤„ç†çš„æ–‡æœ¬ä¿¡æ¯"""
        return getattr(self, '_last_text_info', None)

    def add_situations(self, situations_and_advice):
        """Add financial situations and their corresponding advice. Parameter is a list of tuples (situation, rec)"""

        situations = []
        advice = []
        ids = []
        embeddings = []

        offset = self.situation_collection.count()

        for i, (situation, recommendation) in enumerate(situations_and_advice):
            situations.append(situation)
            advice.append(recommendation)
            ids.append(str(offset + i))
            embeddings.append(self.get_embedding(situation))

        self.situation_collection.add(
            documents=situations,
            metadatas=[{"recommendation": rec} for rec in advice],
            embeddings=embeddings,
            ids=ids,
        )

    def get_memories(self, current_situation, n_matches=1):
        """Find matching recommendations using embeddings with smart truncation handling"""
        
        # è·å–å½“å‰æƒ…å†µçš„embedding
        query_embedding = self.get_embedding(current_situation)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå‘é‡ï¼ˆè®°å¿†åŠŸèƒ½è¢«ç¦ç”¨æˆ–å‡ºé”™ï¼‰
        if all(x == 0.0 for x in query_embedding):
            logger.debug(f" æŸ¥è¯¢embeddingä¸ºç©ºå‘é‡ï¼Œè¿”å›ç©ºç»“æœ")
            return []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡ŒæŸ¥è¯¢
        collection_count = self.situation_collection.count()
        if collection_count == 0:
            logger.debug(f" è®°å¿†åº“ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ")
            return []
        
        # è°ƒæ•´æŸ¥è¯¢æ•°é‡ï¼Œä¸èƒ½è¶…è¿‡é›†åˆä¸­çš„æ–‡æ¡£æ•°é‡
        actual_n_matches = min(n_matches, collection_count)
        
        try:
            # æ‰§è¡Œç›¸ä¼¼åº¦æŸ¥è¯¢
            results = self.situation_collection.query(
                query_embeddings=[query_embedding],
                n_results=actual_n_matches
            )
            
            # å¤„ç†æŸ¥è¯¢ç»“æœ
            memories = []
            if results and 'documents' in results and results['documents']:
                documents = results['documents'][0]
                metadatas = results.get('metadatas', [[]])[0]
                distances = results.get('distances', [[]])[0]
                
                for i, doc in enumerate(documents):
                    metadata = metadatas[i] if i < len(metadatas) else {}
                    distance = distances[i] if i < len(distances) else 1.0
                    
                    memory_item = {
                        'situation': doc,
                        'recommendation': metadata.get('recommendation', ''),
                        'similarity': 1.0 - distance,  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                        'distance': distance
                    }
                    memories.append(memory_item)
                
                # è®°å½•æŸ¥è¯¢ä¿¡æ¯
                if hasattr(self, '_last_text_info') and self._last_text_info.get('was_truncated'):
                    logger.info(f" æˆªæ–­æ–‡æœ¬æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ°{len(memories)}ä¸ªç›¸å…³è®°å¿†")
                    logger.debug(f" åŸæ–‡é•¿åº¦: {self._last_text_info['original_length']}, "
                               f"å¤„ç†åé•¿åº¦: {self._last_text_info['processed_length']}")
                else:
                    logger.debug(f" è®°å¿†æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ°{len(memories)}ä¸ªç›¸å…³è®°å¿†")
            
            return memories
            
        except Exception as e:
            logger.error(f" è®°å¿†æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return []

    def get_cache_info(self):
        """è·å–ç¼“å­˜ç›¸å…³ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•å’Œç›‘æ§"""
        info = {
            'collection_count': self.situation_collection.count(),
            'client_status': 'enabled' if self.client != "DISABLED" else 'disabled',
            'embedding_model': self.embedding,
            'provider': self.llm_provider
        }
        
        # æ·»åŠ æœ€åä¸€æ¬¡æ–‡æœ¬å¤„ç†ä¿¡æ¯
        if hasattr(self, '_last_text_info'):
            info['last_text_processing'] = self._last_text_info
            
        return info


if __name__ == "__main__":
    # Example usage
    matcher = FinancialSituationMemory()

    # Example data
    example_data = [
        (
            "High inflation rate with rising interest rates and declining consumer spending",
            "Consider defensive sectors like consumer staples and utilities. Review fixed-income portfolio duration.",
        ),
        (
            "Tech sector showing high volatility with increasing institutional selling pressure",
            "Reduce exposure to high-growth tech stocks. Look for value opportunities in established tech companies with strong cash flows.",
        ),
        (
            "Strong dollar affecting emerging markets with increasing forex volatility",
            "Hedge currency exposure in international positions. Consider reducing allocation to emerging market debt.",
        ),
        (
            "Market showing signs of sector rotation with rising yields",
            "Rebalance portfolio to maintain target allocations. Consider increasing exposure to sectors benefiting from higher rates.",
        ),
    ]

    # Add the example situations and recommendations
    matcher.add_situations(example_data)

    # Example query
    current_situation = """
    Market showing increased volatility in tech sector, with institutional investors 
    reducing positions and rising interest rates affecting growth stock valuations
    """

    try:
        recommendations = matcher.get_memories(current_situation, n_matches=2)

        for i, rec in enumerate(recommendations, 1):
            logger.info(f"\nMatch {i}:")
            logger.info(f"Similarity Score: {rec.get('similarity', 0):.2f}")
            logger.info(f"Matched Situation: {rec.get('situation', '')}")
            logger.info(f"Recommendation: {rec.get('recommendation', '')}")

    except Exception as e:
        logger.error(f"Error during recommendation: {str(e)}")
