#!/usr/bin/env python3
"""
ä¸­å›½è´¢ç»æ•°æ®èšåˆå·¥å…·
ç”±äºå¾®åšAPIç”³è¯·å›°éš¾ä¸”åŠŸèƒ½å—é™ï¼Œé‡‡ç”¨å¤šæºæ•°æ®èšåˆçš„æ–¹å¼
"""

import requests
import json
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import re
from bs4 import BeautifulSoup
import pandas as pd

# æ•°æ®ç¼“å­˜ï¼šé¿å…çŸ­æ—¶é—´å†…é‡å¤è·å–ç›¸åŒæ•°æ®
_social_sentiment_cache = {}
_social_sentiment_cache_ttl = 300  # ç¼“å­˜5åˆ†é’Ÿ


class ChineseFinanceDataAggregator:
    """ä¸­å›½è´¢ç»æ•°æ®èšåˆå™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def get_stock_sentiment_summary(self, ticker: str, days: int = 7) -> Dict:
        """
        è·å–è‚¡ç¥¨æƒ…ç»ªåˆ†ææ±‡æ€»
        æ•´åˆå¤šä¸ªå¯è·å–çš„ä¸­å›½è´¢ç»æ•°æ®æº
        """
        try:
            # 1. è·å–è´¢ç»æ–°é—»æƒ…ç»ª
            news_sentiment = self._get_finance_news_sentiment(ticker, days)
            
            # 2. è·å–è‚¡å§è®¨è®ºçƒ­åº¦ (å¦‚æœå¯ä»¥è·å–)
            forum_sentiment = self._get_stock_forum_sentiment(ticker, days)
            
            # 3. è·å–è´¢ç»åª’ä½“æŠ¥é“
            media_sentiment = self._get_media_coverage_sentiment(ticker, days)
            
            # 4. ç»¼åˆåˆ†æ
            overall_sentiment = self._calculate_overall_sentiment(
                news_sentiment, forum_sentiment, media_sentiment
            )
            
            return {
                'ticker': ticker,
                'analysis_period': f'{days} days',
                'overall_sentiment': overall_sentiment,
                'news_sentiment': news_sentiment,
                'forum_sentiment': forum_sentiment,
                'media_sentiment': media_sentiment,
                'summary': self._generate_sentiment_summary(overall_sentiment),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                'ticker': ticker,
                'error': f'æ•°æ®è·å–å¤±è´¥: {str(e)}',
                'fallback_message': 'ç”±äºä¸­å›½ç¤¾äº¤åª’ä½“APIé™åˆ¶ï¼Œå»ºè®®ä½¿ç”¨è´¢ç»æ–°é—»å’ŒåŸºæœ¬é¢åˆ†æä½œä¸ºä¸»è¦å‚è€ƒ',
                'timestamp': datetime.now().isoformat()
            }
    
    def _get_finance_news_sentiment(self, ticker: str, days: int) -> Dict:
        """è·å–è´¢ç»æ–°é—»æƒ…ç»ªåˆ†æ"""
        try:
            # æœç´¢ç›¸å…³æ–°é—»æ ‡é¢˜å’Œå†…å®¹
            company_name = self._get_company_chinese_name(ticker)
            search_terms = [ticker, company_name] if company_name else [ticker]
            
            news_items = []
            for term in search_terms:
                # è¿™é‡Œå¯ä»¥é›†æˆå¤šä¸ªæ–°é—»æº
                items = self._search_finance_news(term, days)
                news_items.extend(items)
            
            # ç®€å•çš„æƒ…ç»ªåˆ†æ
            positive_count = 0
            negative_count = 0
            neutral_count = 0
            
            for item in news_items:
                sentiment = self._analyze_text_sentiment(item.get('title', '') + ' ' + item.get('content', ''))
                if sentiment > 0.1:
                    positive_count += 1
                elif sentiment < -0.1:
                    negative_count += 1
                else:
                    neutral_count += 1
            
            total = len(news_items)
            if total == 0:
                return {'sentiment_score': 0, 'confidence': 0, 'news_count': 0}
            
            sentiment_score = (positive_count - negative_count) / total
            
            return {
                'sentiment_score': sentiment_score,
                'positive_ratio': positive_count / total,
                'negative_ratio': negative_count / total,
                'neutral_ratio': neutral_count / total,
                'news_count': total,
                'confidence': min(total / 10, 1.0)  # æ–°é—»æ•°é‡è¶Šå¤šï¼Œç½®ä¿¡åº¦è¶Šé«˜
            }
            
        except Exception as e:
            return {'error': str(e), 'sentiment_score': 0, 'confidence': 0}
    
    def _get_stock_forum_sentiment(self, ticker: str, days: int) -> Dict:
        """è·å–è‚¡ç¥¨è®ºå›è®¨è®ºæƒ…ç»ª - ä½¿ç”¨AKShareè·å–ä¸œæ–¹è´¢å¯Œè‚¡å§æ•°æ®"""
        from tradingagents.utils.logging_manager import get_logger
        logger = get_logger('agents')

        try:
            logger.info(f"[è‚¡å§æƒ…ç»ª] å¼€å§‹è·å– {ticker} çš„ä¸œæ–¹è´¢å¯Œè‚¡å§è¯„è®ºæ•°æ®")

            # ä½¿ç”¨AKShareè·å–ä¸œæ–¹è´¢å¯Œè‚¡å§è¯„è®º
            import akshare as ak

            # æ¸…ç†è‚¡ç¥¨ä»£ç æ ¼å¼
            clean_ticker = ticker.replace('.SH', '').replace('.SZ', '').replace('.SS', '')\
                                .replace('.HK', '').replace('.XSHE', '').replace('.XSHG', '')

            logger.info(f"[è‚¡å§æƒ…ç»ª] å¤„ç†åçš„è‚¡ç¥¨ä»£ç : {clean_ticker}")

            # è·å–è‚¡å§è¯„è®ºæ•°æ®ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
            import threading

            result = [None]
            exception = [None]

            def fetch_comments():
                try:
                    # å°è¯•è·å–ä¸ªè‚¡åƒè‚¡åƒè¯„æ•°æ®ï¼ˆåŒ…å«æŠ•èµ„è€…æƒ…ç»ªï¼‰
                    result[0] = ak.stock_comment_em()
                except Exception as e:
                    exception[0] = e

            # å¯åŠ¨çº¿ç¨‹
            thread = threading.Thread(target=fetch_comments)
            thread.daemon = True
            thread.start()

            # ç­‰å¾…60ç§’
            thread.join(timeout=60)

            if thread.is_alive():
                logger.warning(f"[è‚¡å§æƒ…ç»ª] è·å–è‚¡å§æ•°æ®è¶…æ—¶ï¼ˆ60ç§’ï¼‰: {ticker}")
                raise Exception(f"è‚¡å§æ•°æ®è·å–è¶…æ—¶: {ticker}")
            elif exception[0]:
                raise exception[0]
            else:
                comment_df = result[0]

            if comment_df is not None and not comment_df.empty:
                # æŸ¥æ‰¾å½“å‰è‚¡ç¥¨çš„è¯„è®ºæ•°æ®
                matching_rows = comment_df[comment_df['ä»£ç '].astype(str).str.contains(clean_ticker, na=False)]

                if not matching_rows.empty:
                    row = matching_rows.iloc[0]

                    # è§£ææƒ…ç»ªæ•°æ®
                    # ä¸œæ–¹è´¢å¯Œåƒè‚¡åƒè¯„åŒ…å«ï¼šç»¼åˆè¯„åˆ†ã€ä¸Šæ¶¨æ¦‚ç‡ã€ç›ˆåˆ©èƒ½åŠ›ç­‰æŒ‡æ ‡
                    sentiment_score = 0.5  # é»˜è®¤ä¸­æ€§
                    discussion_count = 0

                    # å°è¯•ä»è¯„åˆ†ä¸­æå–æƒ…ç»ª
                    if 'ç»¼åˆè¯„åˆ†' in row:
                        try:
                            score = float(row['ç»¼åˆè¯„åˆ†'])
                            sentiment_score = score / 100  # å‡è®¾è¯„åˆ†0-100ï¼Œè½¬æ¢ä¸º0-1
                        except:
                            pass

                    logger.info(f"[è‚¡å§æƒ…ç»ª] æˆåŠŸè·å– {ticker} çš„è‚¡å§æƒ…ç»ªæ•°æ®ï¼Œæƒ…ç»ªè¯„åˆ†: {sentiment_score:.2f}")

                    return {
                        'sentiment_score': sentiment_score,
                        'discussion_count': discussion_count,
                        'hot_topics': [],
                        'note': 'æ•°æ®æ¥æºï¼šä¸œæ–¹è´¢å¯Œè‚¡å§ï¼ˆåƒè‚¡åƒè¯„ï¼‰',
                        'confidence': 0.7,  # AKShare APIæ•°æ®å¯ä¿¡åº¦è¾ƒé«˜
                        'source': 'eastmoney_guba'
                    }
                else:
                    logger.warning(f"[è‚¡å§æƒ…ç»ª] æœªæ‰¾åˆ° {ticker} çš„è‚¡å§è¯„è®ºæ•°æ®")
                    return {
                        'sentiment_score': 0.5,
                        'discussion_count': 0,
                        'hot_topics': [],
                        'note': 'æœªæ‰¾åˆ°è¯¥è‚¡ç¥¨çš„è‚¡å§è®¨è®ºæ•°æ®',
                        'confidence': 0,
                        'source': 'eastmoney_guba'
                    }
            else:
                logger.warning(f"[è‚¡å§æƒ…ç»ª] è‚¡å§è¯„è®ºæ•°æ®ä¸ºç©º")
                return {
                    'sentiment_score': 0.5,
                    'discussion_count': 0,
                    'hot_topics': [],
                    'note': 'è‚¡å§è¯„è®ºæ•°æ®æš‚æ—¶ä¸å¯ç”¨',
                    'confidence': 0,
                    'source': 'eastmoney_guba'
                }

        except Exception as e:
            logger.error(f"[è‚¡å§æƒ…ç»ª] è·å–è‚¡å§æ•°æ®å¤±è´¥: {e}")
            return {
                'sentiment_score': 0.5,
                'discussion_count': 0,
                'hot_topics': [],
                'note': f'è‚¡ç¥¨è®ºå›æ•°æ®è·å–å¤±è´¥: {str(e)}',
                'confidence': 0,
                'source': 'eastmoney_guba_error'
            }
    
    def _get_media_coverage_sentiment(self, ticker: str, days: int) -> Dict:
        """è·å–åª’ä½“æŠ¥é“æƒ…ç»ª - æ•´åˆé›ªçƒï¼ˆXueqiuï¼‰æ•°æ®"""
        from tradingagents.utils.logging_manager import get_logger
        logger = get_logger('agents')

        try:
            logger.info(f"[åª’ä½“æƒ…ç»ª] å¼€å§‹è·å– {ticker} çš„åª’ä½“æŠ¥é“å’Œé›ªçƒæƒ…ç»ªæ•°æ®")

            # ğŸ†• ä½¿ç”¨AKShareè·å–é›ªçƒæƒ…ç»ªæ•°æ®
            try:
                from tradingagents.dataflows.akshare_utils import get_xueqiu_stock_sentiment
                xueqiu_data = get_xueqiu_stock_sentiment(ticker)

                if xueqiu_data and 'error' not in xueqiu_data:
                    sentiment_score = xueqiu_data.get('sentiment_score', 0.5)
                    confidence = xueqiu_data.get('confidence', 0)
                    follow_count = xueqiu_data.get('follow_count', 0)
                    tweet_count = xueqiu_data.get('tweet_count', 0)

                    logger.info(f"[åª’ä½“æƒ…ç»ª]  é›ªçƒæ•°æ®è·å–æˆåŠŸ: æƒ…ç»ª={sentiment_score:.2f}, å…³æ³¨={follow_count}, è®¨è®º={tweet_count}")

                    return {
                        'sentiment_score': sentiment_score,
                        'coverage_count': 1,  # é›ªçƒæ•°æ®æº
                        'confidence': confidence,
                        'source': 'xueqiu',
                        'follow_count': follow_count,
                        'tweet_count': tweet_count,
                        'note': xueqiu_data.get('note', 'åŸºäºé›ªçƒå¹³å°æ•°æ®è®¡ç®—æƒ…ç»ª')
                    }
                else:
                    error_msg = xueqiu_data.get('error', 'æœªçŸ¥é”™è¯¯') if xueqiu_data else 'æ•°æ®ä¸ºç©º'
                    logger.warning(f"[åª’ä½“æƒ…ç»ª]  é›ªçƒæ•°æ®è·å–å¤±è´¥: {error_msg}")
                    return {'sentiment_score': 0.5, 'coverage_count': 0, 'confidence': 0, 'error': error_msg}

            except Exception as e:
                logger.error(f"[åª’ä½“æƒ…ç»ª]  é›ªçƒæ•°æ®è·å–å¼‚å¸¸: {e}")
                return {'sentiment_score': 0.5, 'coverage_count': 0, 'confidence': 0, 'error': str(e)}

        except Exception as e:
            logger.error(f"[åª’ä½“æƒ…ç»ª]  æ•´ä½“è·å–å¤±è´¥: {e}")
            return {'error': str(e), 'sentiment_score': 0.5, 'confidence': 0}
    
    def _search_finance_news(self, search_term: str, days: int) -> List[Dict]:
        """æœç´¢è´¢ç»æ–°é—» (ç¤ºä¾‹å®ç°)"""
        # è¿™é‡Œå¯ä»¥é›†æˆå¤šä¸ªæ–°é—»æºçš„APIæˆ–RSS
        # ä¾‹å¦‚ï¼šè´¢è”ç¤¾ã€æ–°æµªè´¢ç»ã€ä¸œæ–¹è´¢å¯Œç­‰
        
        # æ¨¡æ‹Ÿè¿”å›æ•°æ®ç»“æ„
        return [
            {
                'title': f'{search_term}ç›¸å…³è´¢ç»æ–°é—»æ ‡é¢˜',
                'content': 'æ–°é—»å†…å®¹æ‘˜è¦...',
                'source': 'è´¢è”ç¤¾',
                'publish_time': datetime.now().isoformat(),
                'url': 'https://example.com/news/1'
            }
        ]
    
    def _get_media_coverage(self, ticker: str, days: int) -> List[Dict]:
        """è·å–åª’ä½“æŠ¥é“ (ç¤ºä¾‹å®ç°)"""
        # å¯ä»¥é›†æˆGoogle News APIæˆ–å…¶ä»–æ–°é—»èšåˆæœåŠ¡
        return []
    
    def _analyze_text_sentiment(self, text: str) -> float:
        """ç®€å•çš„ä¸­æ–‡æ–‡æœ¬æƒ…ç»ªåˆ†æ"""
        if not text:
            return 0
        
        # ç®€å•çš„å…³é”®è¯æƒ…ç»ªåˆ†æ
        positive_words = ['ä¸Šæ¶¨', 'å¢é•¿', 'åˆ©å¥½', 'çœ‹å¥½', 'ä¹°å…¥', 'æ¨è', 'å¼ºåŠ¿', 'çªç ´', 'åˆ›æ–°é«˜']
        negative_words = ['ä¸‹è·Œ', 'ä¸‹é™', 'åˆ©ç©º', 'çœ‹ç©º', 'å–å‡º', 'é£é™©', 'è·Œç ´', 'åˆ›æ–°ä½', 'äºæŸ']
        
        positive_count = sum(1 for word in positive_words if word in text)
        negative_count = sum(1 for word in negative_words if word in text)
        
        if positive_count + negative_count == 0:
            return 0
        
        return (positive_count - negative_count) / (positive_count + negative_count)
    
    def _get_company_chinese_name(self, ticker: str) -> Optional[str]:
        """è·å–å…¬å¸ä¸­æ–‡åç§°"""
        # ç®€å•çš„æ˜ å°„è¡¨ï¼Œå®é™…å¯ä»¥ä»æ•°æ®åº“æˆ–APIè·å–
        name_mapping = {
            'AAPL': 'è‹¹æœ',
            'TSLA': 'ç‰¹æ–¯æ‹‰',
            'NVDA': 'è‹±ä¼Ÿè¾¾',
            'MSFT': 'å¾®è½¯',
            'GOOGL': 'è°·æ­Œ',
            'AMZN': 'äºšé©¬é€Š'
        }
        return name_mapping.get(ticker.upper())
    
    def _calculate_overall_sentiment(self, news_sentiment: Dict, forum_sentiment: Dict, media_sentiment: Dict) -> Dict:
        """è®¡ç®—ç»¼åˆæƒ…ç»ªåˆ†æ"""
        # æ ¹æ®å„æ•°æ®æºçš„ç½®ä¿¡åº¦åŠ æƒè®¡ç®—
        news_weight = news_sentiment.get('confidence', 0)
        forum_weight = forum_sentiment.get('confidence', 0)
        media_weight = media_sentiment.get('confidence', 0)
        
        total_weight = news_weight + forum_weight + media_weight
        
        if total_weight == 0:
            return {'sentiment_score': 0, 'confidence': 0, 'level': 'neutral'}
        
        weighted_sentiment = (
            news_sentiment.get('sentiment_score', 0) * news_weight +
            forum_sentiment.get('sentiment_score', 0) * forum_weight +
            media_sentiment.get('sentiment_score', 0) * media_weight
        ) / total_weight
        
        # ç¡®å®šæƒ…ç»ªç­‰çº§
        if weighted_sentiment > 0.3:
            level = 'very_positive'
        elif weighted_sentiment > 0.1:
            level = 'positive'
        elif weighted_sentiment > -0.1:
            level = 'neutral'
        elif weighted_sentiment > -0.3:
            level = 'negative'
        else:
            level = 'very_negative'
        
        return {
            'sentiment_score': weighted_sentiment,
            'confidence': total_weight / 3,  # å¹³å‡ç½®ä¿¡åº¦
            'level': level
        }
    
    def _generate_sentiment_summary(self, overall_sentiment: Dict) -> str:
        """ç”Ÿæˆæƒ…ç»ªåˆ†ææ‘˜è¦"""
        level = overall_sentiment.get('level', 'neutral')
        score = overall_sentiment.get('sentiment_score', 0)
        confidence = overall_sentiment.get('confidence', 0)
        
        level_descriptions = {
            'very_positive': 'éå¸¸ç§¯æ',
            'positive': 'ç§¯æ',
            'neutral': 'ä¸­æ€§',
            'negative': 'æ¶ˆæ',
            'very_negative': 'éå¸¸æ¶ˆæ'
        }
        
        description = level_descriptions.get(level, 'ä¸­æ€§')
        confidence_level = 'é«˜' if confidence > 0.7 else 'ä¸­' if confidence > 0.3 else 'ä½'
        
        return f"å¸‚åœºæƒ…ç»ª: {description} (è¯„åˆ†: {score:.2f}, ç½®ä¿¡åº¦: {confidence_level})"


def get_chinese_social_sentiment(ticker: str, curr_date: str) -> str:
    """
    è·å–ä¸­å›½ç¤¾äº¤åª’ä½“æƒ…ç»ªåˆ†æçš„ä¸»è¦æ¥å£å‡½æ•°
    æ·»åŠ ç¼“å­˜æœºåˆ¶ï¼Œé¿å…çŸ­æ—¶é—´å†…é‡å¤è·å–ç›¸åŒæ•°æ®
    """
    from ..utils.logging_init import get_logger
    logger = get_logger('agents')

    # æ£€æŸ¥ç¼“å­˜
    cache_key = f"{ticker}_{curr_date}"
    current_time = time.time()

    if cache_key in _social_sentiment_cache:
        cached_data, cached_time = _social_sentiment_cache[cache_key]
        if current_time - cached_time < _social_sentiment_cache_ttl:
            logger.info(f" [ç¤¾äº¤æƒ…ç»ªç¼“å­˜] ä½¿ç”¨ç¼“å­˜æ•°æ®: {ticker}, ç¼“å­˜æ—¶é—´: {int(current_time - cached_time)}ç§’å‰")
            return cached_data

    logger.info(f" [ç¤¾äº¤æƒ…ç»ª] è·å–æ–°æ•°æ®: {ticker}")

    aggregator = ChineseFinanceDataAggregator()

    try:
        # è·å–æƒ…ç»ªåˆ†ææ•°æ®
        sentiment_data = aggregator.get_stock_sentiment_summary(ticker, days=7)

        # æ ¼å¼åŒ–è¾“å‡º
        if 'error' in sentiment_data:
            result = f"""
ä¸­å›½å¸‚åœºæƒ…ç»ªåˆ†ææŠ¥å‘Š - {ticker}
åˆ†ææ—¥æœŸ: {curr_date}

 æ•°æ®è·å–é™åˆ¶è¯´æ˜:
{sentiment_data.get('fallback_message', 'æ•°æ®è·å–é‡åˆ°æŠ€æœ¯é™åˆ¶')}

å»ºè®®:
1. é‡ç‚¹å…³æ³¨è´¢ç»æ–°é—»å’ŒåŸºæœ¬é¢åˆ†æ
2. å‚è€ƒå®˜æ–¹è´¢æŠ¥å’Œä¸šç»©æŒ‡å¯¼
3. å…³æ³¨è¡Œä¸šæ”¿ç­–å’Œç›‘ç®¡åŠ¨æ€
4. è€ƒè™‘å›½é™…å¸‚åœºæƒ…ç»ªå¯¹ä¸­æ¦‚è‚¡çš„å½±å“

æ³¨: ç”±äºä¸­å›½ç¤¾äº¤åª’ä½“å¹³å°APIé™åˆ¶ï¼Œå½“å‰ä¸»è¦ä¾èµ–å…¬å¼€è´¢ç»æ•°æ®æºè¿›è¡Œåˆ†æã€‚
"""
        else:
            overall = sentiment_data.get('overall_sentiment', {})
            news = sentiment_data.get('news_sentiment', {})

            result = f"""
ä¸­å›½å¸‚åœºæƒ…ç»ªåˆ†ææŠ¥å‘Š - {ticker}
åˆ†ææ—¥æœŸ: {curr_date}
åˆ†æå‘¨æœŸ: {sentiment_data.get('analysis_period', '7å¤©')}

 ç»¼åˆæƒ…ç»ªè¯„ä¼°:
{sentiment_data.get('summary', 'æ•°æ®ä¸è¶³')}

 è´¢ç»æ–°é—»æƒ…ç»ª:
- æƒ…ç»ªè¯„åˆ†: {news.get('sentiment_score', 0):.2f}
- æ­£é¢æ–°é—»æ¯”ä¾‹: {news.get('positive_ratio', 0):.1%}
- è´Ÿé¢æ–°é—»æ¯”ä¾‹: {news.get('negative_ratio', 0):.1%}
- æ–°é—»æ•°é‡: {news.get('news_count', 0)}æ¡

 æŠ•èµ„å»ºè®®:
åŸºäºå½“å‰å¯è·å–çš„ä¸­å›½å¸‚åœºæ•°æ®ï¼Œå»ºè®®æŠ•èµ„è€…:
1. å¯†åˆ‡å…³æ³¨å®˜æ–¹è´¢ç»åª’ä½“æŠ¥é“
2. é‡è§†åŸºæœ¬é¢åˆ†æå’Œè´¢åŠ¡æ•°æ®
3. è€ƒè™‘æ”¿ç­–ç¯å¢ƒå¯¹è‚¡ä»·çš„å½±å“
4. å…³æ³¨å›½é™…å¸‚åœºåŠ¨æ€

 æ•°æ®è¯´æ˜:
ç”±äºä¸­å›½ç¤¾äº¤åª’ä½“å¹³å°APIè·å–é™åˆ¶ï¼Œæœ¬åˆ†æä¸»è¦åŸºäºå…¬å¼€è´¢ç»æ–°é—»æ•°æ®ã€‚
å»ºè®®ç»“åˆå…¶ä»–åˆ†æç»´åº¦è¿›è¡Œç»¼åˆåˆ¤æ–­ã€‚

ç”Ÿæˆæ—¶é—´: {sentiment_data.get('timestamp', datetime.now().isoformat())}
"""

        # ç¼“å­˜ç»“æœ
        _social_sentiment_cache[cache_key] = (result, current_time)
        logger.info(f" [ç¤¾äº¤æƒ…ç»ªç¼“å­˜] ç¼“å­˜æ•°æ®: {ticker}, TTL: {_social_sentiment_cache_ttl}ç§’")

        return result

    except Exception as e:
        result = f"""
ä¸­å›½å¸‚åœºæƒ…ç»ªåˆ†æ - {ticker}
åˆ†ææ—¥æœŸ: {curr_date}

 åˆ†æå¤±è´¥: {str(e)}

 æ›¿ä»£å»ºè®®:
1. æŸ¥çœ‹è´¢ç»æ–°é—»ç½‘ç«™çš„ç›¸å…³æŠ¥é“
2. å…³æ³¨é›ªçƒã€ä¸œæ–¹è´¢å¯Œç­‰æŠ•èµ„ç¤¾åŒºè®¨è®º
3. å‚è€ƒä¸“ä¸šæœºæ„çš„ç ”ç©¶æŠ¥å‘Š
4. é‡ç‚¹åˆ†æåŸºæœ¬é¢å’ŒæŠ€æœ¯é¢æ•°æ®

æ³¨: ä¸­å›½ç¤¾äº¤åª’ä½“æ•°æ®è·å–å­˜åœ¨æŠ€æœ¯é™åˆ¶ï¼Œå»ºè®®ä»¥åŸºæœ¬é¢åˆ†æä¸ºä¸»ã€‚
"""
        # å³ä½¿å¤±è´¥ä¹Ÿç¼“å­˜ç»“æœï¼Œé¿å…é‡å¤å¤±è´¥
        _social_sentiment_cache[cache_key] = (result, current_time)
        return result
