#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Performance Benchmark Script

æµ‹è¯•ä¼˜åŒ–åŠŸèƒ½çš„æ€§èƒ½æå‡æ•ˆæœï¼Œç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Šã€‚

Usage:
    python scripts/benchmark_optimizations.py --mode all
    python scripts/benchmark_optimizations.py --mode cache
    python scripts/benchmark_optimizations.py --mode llm-routing
"""

import os
import sys
import time
import argparse
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
from collections import defaultdict

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv(project_root / ".env")

from tradingagents.utils.logging_init import get_logger
from tradingagents.utils.monitoring_metrics import get_metrics_collector, reset_metrics_collector
from tradingagents.utils.llm_optimization import get_llm_cache_stats, clear_llm_cache
from tradingagents.dataflows.ttl_cache import get_hybrid_cache

logger = get_logger("benchmark")


class BenchmarkRunner:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self):
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "system_info": self._get_system_info(),
            "tests": []
        }
        self.metrics = get_metrics_collector()

    def _get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        import platform
        return {
            "python_version": platform.python_version(),
            "platform": platform.platform(),
            "processor": platform.processor(),
        }

    def run_cache_benchmark(self) -> Dict[str, Any]:
        """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š Cache Performance Benchmark")
        logger.info("="*60)

        from tradingagents.dataflows.data_source_manager import DataSourceManager

        manager = DataSourceManager()
        test_symbol = "000001.SZ"
        start_date = "20240101"
        end_date = "20240131"

        # æ¸…ç©ºç¼“å­˜
        cache = get_hybrid_cache()
        cache.clear()
        logger.info("âœ“ ç¼“å­˜å·²æ¸…ç©º")

        # æµ‹è¯•1: é¦–æ¬¡è¯·æ±‚ï¼ˆæ— ç¼“å­˜ï¼‰
        logger.info("\nğŸ“ æµ‹è¯•1: é¦–æ¬¡è¯·æ±‚ï¼ˆæ— ç¼“å­˜ï¼‰")
        start_time = time.time()
        result1 = manager.get_china_stock_data_unified(test_symbol, start_date, end_date)
        duration_no_cache = time.time() - start_time
        logger.info(f"   è€—æ—¶: {duration_no_cache:.3f}ç§’")

        # æµ‹è¯•2: é‡å¤è¯·æ±‚ï¼ˆåº”å‘½ä¸­ç¼“å­˜ï¼‰
        logger.info("\nğŸ“ æµ‹è¯•2: é‡å¤è¯·æ±‚ï¼ˆå‘½ä¸­ç¼“å­˜ï¼‰")
        start_time = time.time()
        result2 = manager.get_china_stock_data_unified(test_symbol, start_date, end_date)
        duration_with_cache = time.time() - start_time
        logger.info(f"   è€—æ—¶: {duration_with_cache:.3f}ç§’")

        # è®¡ç®—æå‡
        speedup = duration_no_cache / duration_with_cache if duration_with_cache > 0 else 0
        time_saved = duration_no_cache - duration_with_cache

        logger.info(f"\nâœ… ç¼“å­˜æ€§èƒ½æå‡:")
        logger.info(f"   é€Ÿåº¦æå‡: {speedup:.1f}x")
        logger.info(f"   æ—¶é—´èŠ‚çœ: {time_saved:.3f}ç§’ ({time_saved/duration_no_cache*100:.1f}%)")

        return {
            "test_name": "Cache Performance",
            "no_cache_duration": duration_no_cache,
            "with_cache_duration": duration_with_cache,
            "speedup": speedup,
            "time_saved_seconds": time_saved,
            "time_saved_percentage": time_saved / duration_no_cache * 100
        }

    def run_llm_routing_benchmark(self) -> Dict[str, Any]:
        """æµ‹è¯•LLMåˆ†å±‚è·¯ç”±"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š LLM Routing Benchmark")
        logger.info("="*60)

        from tradingagents.utils.llm_router import get_llm_router, LLMTier

        router = get_llm_router()

        # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„Agent
        test_cases = [
            ("trader", "SIMPLE", LLMTier.SMALL),
            ("market", "ROUTINE", LLMTier.MEDIUM),
            ("research_manager", "COMPLEX", LLMTier.LARGE),
        ]

        results = []
        for agent_name, expected_complexity, expected_tier in test_cases:
            logger.info(f"\nğŸ“ æµ‹è¯•Agent: {agent_name}")
            logger.info(f"   é¢„æœŸå¤æ‚åº¦: {expected_complexity}")
            logger.info(f"   é¢„æœŸæ¨¡å‹å±‚çº§: {expected_tier.value}")

            # è·å–åˆ†é…çš„LLM
            llm = router.get_llm_for_agent(agent_name)
            actual_tier = router._get_tier_for_agent(agent_name)

            logger.info(f"   å®é™…æ¨¡å‹å±‚çº§: {actual_tier.value}")
            logger.info(f"   åˆ†é…çš„æ¨¡å‹: {llm.model_name if hasattr(llm, 'model_name') else 'Unknown'}")

            success = (actual_tier == expected_tier)
            logger.info(f"   âœ“ è·¯ç”±æ­£ç¡®" if success else "   âœ— è·¯ç”±é”™è¯¯")

            results.append({
                "agent_name": agent_name,
                "expected_tier": expected_tier.value,
                "actual_tier": actual_tier.value,
                "success": success
            })

        success_rate = sum(1 for r in results if r["success"]) / len(results)

        logger.info(f"\nâœ… è·¯ç”±æˆåŠŸç‡: {success_rate:.1%}")

        return {
            "test_name": "LLM Routing",
            "test_cases": results,
            "success_rate": success_rate
        }

    def run_llm_optimization_benchmark(self) -> Dict[str, Any]:
        """æµ‹è¯•LLMä¼˜åŒ–ï¼ˆä¸Šä¸‹æ–‡è£å‰ª+ç»“æœç¼“å­˜ï¼‰"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š LLM Optimization Benchmark")
        logger.info("="*60)

        from tradingagents.utils.llm_optimization import (
            ContextPruner,
            get_llm_cache,
            optimize_llm_call
        )

        # æµ‹è¯•1: ä¸Šä¸‹æ–‡è£å‰ª
        logger.info("\nğŸ“ æµ‹è¯•1: ä¸Šä¸‹æ–‡è£å‰ª")

        pruner = ContextPruner(max_tokens=1000, truncate_strategy="middle")

        # ç”Ÿæˆé•¿æ–‡æœ¬ï¼ˆçº¦5000 tokensï¼‰
        long_text = """
# å¸‚åœºåˆ†ææŠ¥å‘Š

## æ¦‚è¿°
ä»Šæ—¥Aè‚¡å¸‚åœºè¡¨ç°...(é‡å¤1000æ¬¡)
""" * 1000

        original_tokens = pruner._estimate_tokens(long_text)
        logger.info(f"   åŸå§‹æ–‡æœ¬: {original_tokens} tokens")

        start_time = time.time()
        pruned_text, was_truncated = pruner.truncate(long_text)
        prune_duration = time.time() - start_time

        pruned_tokens = pruner._estimate_tokens(pruned_text)
        logger.info(f"   è£å‰ªå: {pruned_tokens} tokens")
        logger.info(f"   è€—æ—¶: {prune_duration:.3f}ç§’")
        logger.info(f"   TokenèŠ‚çœ: {original_tokens - pruned_tokens} ({(original_tokens - pruned_tokens)/original_tokens*100:.1f}%)")

        # æµ‹è¯•2: ç»“æœç¼“å­˜
        logger.info("\nğŸ“ æµ‹è¯•2: LLMç»“æœç¼“å­˜")

        # æ¸…ç©ºç¼“å­˜
        clear_llm_cache()
        cache = get_llm_cache()

        test_prompt = "åˆ†æ000001.SZçš„æŠ•èµ„ä»·å€¼"
        test_model = "qwen-plus"
        test_result = "å»ºè®®ä¹°å…¥ï¼ŒæŠ€æœ¯é¢å¼ºåŠ¿..."

        # é¦–æ¬¡è®¾ç½®
        start_time = time.time()
        cache.set(test_prompt, test_model, test_result)
        set_duration = time.time() - start_time
        logger.info(f"   è®¾ç½®ç¼“å­˜è€—æ—¶: {set_duration:.6f}ç§’")

        # å‘½ä¸­ç¼“å­˜
        start_time = time.time()
        cached = cache.get(test_prompt, test_model)
        get_duration = time.time() - start_time
        logger.info(f"   è¯»å–ç¼“å­˜è€—æ—¶: {get_duration:.6f}ç§’")

        hit_success = (cached == test_result)
        logger.info(f"   ç¼“å­˜å‘½ä¸­: {'âœ“' if hit_success else 'âœ—'}")

        # è·å–ç»Ÿè®¡
        stats = get_llm_cache_stats()
        logger.info(f"\nâœ… ç¼“å­˜ç»Ÿè®¡:")
        logger.info(f"   å¤§å°: {stats['size']}/{stats['max_size']}")
        logger.info(f"   å‘½ä¸­ç‡: {stats['hit_rate']:.2%}")

        return {
            "test_name": "LLM Optimization",
            "context_pruning": {
                "original_tokens": original_tokens,
                "pruned_tokens": pruned_tokens,
                "tokens_saved": original_tokens - pruned_tokens,
                "reduction_percentage": (original_tokens - pruned_tokens) / original_tokens * 100,
                "duration_seconds": prune_duration
            },
            "result_caching": {
                "set_duration_seconds": set_duration,
                "get_duration_seconds": get_duration,
                "hit_success": hit_success,
                "cache_stats": stats
            }
        }

    def run_monitoring_benchmark(self) -> Dict[str, Any]:
        """æµ‹è¯•ç›‘æ§ç³»ç»Ÿ"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š Monitoring System Benchmark")
        logger.info("="*60)

        # é‡ç½®æŒ‡æ ‡
        reset_metrics_collector()
        metrics = get_metrics_collector()

        # æ¨¡æ‹Ÿå„ç§äº‹ä»¶
        logger.info("\nğŸ“ æ¨¡æ‹Ÿäº‹ä»¶è®°å½•")

        # è®°å½•ç¼“å­˜äº‹ä»¶
        for _ in range(100):
            metrics.record_cache_hit()
        for _ in range(30):
            metrics.record_cache_miss()

        # è®°å½•APIè¯·æ±‚
        for _ in range(50):
            metrics.record_api_request(success=True, duration=0.5)
        for _ in range(5):
            metrics.record_api_request(success=False, duration=2.0)

        # è®°å½•LLMä½¿ç”¨
        metrics.record_llm_usage(tokens=1000, cost=0.04, tier="small")
        metrics.record_llm_usage(tokens=5000, cost=0.2, tier="medium")
        metrics.record_llm_usage(tokens=10000, cost=0.8, tier="large")

        # è·å–æŒ‡æ ‡
        start_time = time.time()
        collected_metrics = metrics.get_metrics()
        collection_duration = time.time() - start_time

        logger.info(f"   æŒ‡æ ‡æ”¶é›†è€—æ—¶: {collection_duration:.6f}ç§’")

        # ç”ŸæˆPrometheusæ ¼å¼
        start_time = time.time()
        prometheus_text = metrics.get_prometheus_format()
        prometheus_duration = time.time() - start_time

        logger.info(f"   Prometheusæ ¼å¼ç”Ÿæˆè€—æ—¶: {prometheus_duration:.6f}ç§’")
        logger.info(f"   Prometheusæ–‡æœ¬é•¿åº¦: {len(prometheus_text)} å­—ç¬¦")

        logger.info(f"\nâœ… ç›‘æ§ç³»ç»Ÿæ€§èƒ½:")
        logger.info(f"   ç¼“å­˜å‘½ä¸­ç‡: {collected_metrics['cache_performance']['hit_rate']:.2%}")
        logger.info(f"   APIæˆåŠŸç‡: {collected_metrics['api_statistics']['success_rate']:.2%}")
        logger.info(f"   LLMæ€»æ¶ˆè€—: {collected_metrics['llm_usage']['total_tokens']} tokens")

        return {
            "test_name": "Monitoring System",
            "collection_duration_seconds": collection_duration,
            "prometheus_generation_duration_seconds": prometheus_duration,
            "prometheus_text_length": len(prometheus_text),
            "metrics_snapshot": collected_metrics
        }

    def run_integration_benchmark(self) -> Dict[str, Any]:
        """é›†æˆæµ‹è¯•ï¼šå®Œæ•´çš„Agentåˆ†ææµç¨‹"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š Integration Benchmark (Full Agent Analysis)")
        logger.info("="*60)

        from tradingagents.graph.trading_graph import TradingAgentsGraph
        from tradingagents.default_config import DEFAULT_CONFIG

        # åˆå§‹åŒ–TradingGraph
        logger.info("\nğŸ“ åˆå§‹åŒ–TradingAgentsGraph")
        start_time = time.time()
        graph = TradingAgentsGraph(config=DEFAULT_CONFIG)
        init_duration = time.time() - start_time
        logger.info(f"   åˆå§‹åŒ–è€—æ—¶: {init_duration:.3f}ç§’")

        # æµ‹è¯•ç¬¦å·
        test_symbol = "000001.SZ"
        test_date = "2024-01-15"

        # æ¸…ç©ºç¼“å­˜
        clear_llm_cache()
        logger.info("\nğŸ“ æµ‹è¯•1: é¦–æ¬¡åˆ†æï¼ˆæ— ç¼“å­˜ï¼‰")

        start_time = time.time()
        try:
            final_state, processed_signal = graph.propagate(test_symbol, test_date)
            first_run_duration = time.time() - start_time
            first_run_success = True
            logger.info(f"   âœ“ åˆ†æå®Œæˆï¼Œè€—æ—¶: {first_run_duration:.3f}ç§’")
        except Exception as e:
            first_run_duration = time.time() - start_time
            first_run_success = False
            logger.error(f"   âœ— åˆ†æå¤±è´¥: {e}")

        # é‡å¤åˆ†æï¼ˆåº”å‘½ä¸­ç¼“å­˜ï¼‰
        logger.info("\nğŸ“ æµ‹è¯•2: é‡å¤åˆ†æï¼ˆå‘½ä¸­ç¼“å­˜ï¼‰")

        start_time = time.time()
        try:
            final_state2, processed_signal2 = graph.propagate(test_symbol, test_date)
            second_run_duration = time.time() - start_time
            second_run_success = True
            logger.info(f"   âœ“ åˆ†æå®Œæˆï¼Œè€—æ—¶: {second_run_duration:.3f}ç§’")
        except Exception as e:
            second_run_duration = time.time() - start_time
            second_run_success = False
            logger.error(f"   âœ— åˆ†æå¤±è´¥: {e}")

        # è®¡ç®—æå‡
        if first_run_success and second_run_success:
            speedup = first_run_duration / second_run_duration if second_run_duration > 0 else 0
            time_saved = first_run_duration - second_run_duration

            logger.info(f"\nâœ… é›†æˆæµ‹è¯•æ€§èƒ½æå‡:")
            logger.info(f"   é€Ÿåº¦æå‡: {speedup:.1f}x")
            logger.info(f"   æ—¶é—´èŠ‚çœ: {time_saved:.3f}ç§’ ({time_saved/first_run_duration*100:.1f}%)")

        # è·å–æœ€ç»ˆæŒ‡æ ‡
        final_metrics = self.metrics.get_metrics()

        return {
            "test_name": "Integration (Full Agent Analysis)",
            "initialization_duration_seconds": init_duration,
            "first_run_duration_seconds": first_run_duration,
            "first_run_success": first_run_success,
            "second_run_duration_seconds": second_run_duration,
            "second_run_success": second_run_success,
            "speedup": speedup if first_run_success and second_run_success else 0,
            "time_saved_seconds": time_saved if first_run_success and second_run_success else 0,
            "final_metrics": final_metrics
        }

    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        logger.info("\n" + "="*60)
        logger.info("ğŸš€ Running All Benchmarks")
        logger.info("="*60)

        # è¿è¡Œå„é¡¹æµ‹è¯•
        self.results["tests"].append(self.run_cache_benchmark())
        self.results["tests"].append(self.run_llm_routing_benchmark())
        self.results["tests"].append(self.run_llm_optimization_benchmark())
        self.results["tests"].append(self.run_monitoring_benchmark())

        # é›†æˆæµ‹è¯•ï¼ˆå¯é€‰ï¼Œè€—æ—¶è¾ƒé•¿ï¼‰
        run_integration = os.getenv("BENCHMARK_RUN_INTEGRATION", "false").lower() == "true"
        if run_integration:
            self.results["tests"].append(self.run_integration_benchmark())
        else:
            logger.info("\nâ­ï¸ è·³è¿‡é›†æˆæµ‹è¯•ï¼ˆè®¾ç½® BENCHMARK_RUN_INTEGRATION=true å¯ç”¨ï¼‰")

    def generate_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“„ Generating Performance Report")
        logger.info("="*60)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("benchmark_results")
        output_dir.mkdir(exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        json_file = output_dir / f"benchmark_{timestamp}.json"
        md_file = output_dir / f"benchmark_{timestamp}.md"

        # ä¿å­˜JSON
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ“ JSONæŠ¥å‘Šå·²ä¿å­˜: {json_file}")

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_content = self._generate_markdown_report()

        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)

        logger.info(f"âœ“ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {md_file}")

        return str(md_file)

    def _generate_markdown_report(self) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        lines = []

        lines.append("# HiddenGem Backend æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        lines.append("")
        lines.append(f"**æµ‹è¯•æ—¶é—´**: {self.results['timestamp']}")
        lines.append(f"**ç³»ç»Ÿä¿¡æ¯**: {self.results['system_info']['platform']}")
        lines.append(f"**Pythonç‰ˆæœ¬**: {self.results['system_info']['python_version']}")
        lines.append("")
        lines.append("---")
        lines.append("")

        # é€ä¸ªæµ‹è¯•ç”ŸæˆæŠ¥å‘Š
        for test in self.results["tests"]:
            test_name = test.get("test_name", "Unknown")
            lines.append(f"## {test_name}")
            lines.append("")

            if test_name == "Cache Performance":
                lines.append(f"- **æ— ç¼“å­˜è€—æ—¶**: {test['no_cache_duration']:.3f}ç§’")
                lines.append(f"- **æœ‰ç¼“å­˜è€—æ—¶**: {test['with_cache_duration']:.3f}ç§’")
                lines.append(f"- **é€Ÿåº¦æå‡**: {test['speedup']:.1f}x")
                lines.append(f"- **æ—¶é—´èŠ‚çœ**: {test['time_saved_seconds']:.3f}ç§’ ({test['time_saved_percentage']:.1f}%)")

            elif test_name == "LLM Routing":
                lines.append(f"- **æˆåŠŸç‡**: {test['success_rate']:.1%}")
                lines.append("")
                lines.append("| Agent | é¢„æœŸå±‚çº§ | å®é™…å±‚çº§ | ç»“æœ |")
                lines.append("|-------|---------|---------|------|")
                for case in test["test_cases"]:
                    result = "âœ“" if case["success"] else "âœ—"
                    lines.append(f"| {case['agent_name']} | {case['expected_tier']} | {case['actual_tier']} | {result} |")

            elif test_name == "LLM Optimization":
                ctx = test["context_pruning"]
                cache = test["result_caching"]

                lines.append("### ä¸Šä¸‹æ–‡è£å‰ª")
                lines.append(f"- **åŸå§‹Tokens**: {ctx['original_tokens']}")
                lines.append(f"- **è£å‰ªåTokens**: {ctx['pruned_tokens']}")
                lines.append(f"- **èŠ‚çœ**: {ctx['tokens_saved']} ({ctx['reduction_percentage']:.1f}%)")
                lines.append(f"- **è€—æ—¶**: {ctx['duration_seconds']:.6f}ç§’")
                lines.append("")

                lines.append("### ç»“æœç¼“å­˜")
                lines.append(f"- **è®¾ç½®ç¼“å­˜è€—æ—¶**: {cache['set_duration_seconds']:.6f}ç§’")
                lines.append(f"- **è¯»å–ç¼“å­˜è€—æ—¶**: {cache['get_duration_seconds']:.6f}ç§’")
                lines.append(f"- **å‘½ä¸­æˆåŠŸ**: {'âœ“' if cache['hit_success'] else 'âœ—'}")
                lines.append(f"- **ç¼“å­˜å‘½ä¸­ç‡**: {cache['cache_stats']['hit_rate']:.2%}")

            elif test_name == "Monitoring System":
                lines.append(f"- **æŒ‡æ ‡æ”¶é›†è€—æ—¶**: {test['collection_duration_seconds']:.6f}ç§’")
                lines.append(f"- **Prometheusç”Ÿæˆè€—æ—¶**: {test['prometheus_generation_duration_seconds']:.6f}ç§’")
                lines.append(f"- **ç¼“å­˜å‘½ä¸­ç‡**: {test['metrics_snapshot']['cache_performance']['hit_rate']:.2%}")
                lines.append(f"- **APIæˆåŠŸç‡**: {test['metrics_snapshot']['api_statistics']['success_rate']:.2%}")

            elif test_name == "Integration (Full Agent Analysis)":
                lines.append(f"- **åˆå§‹åŒ–è€—æ—¶**: {test['initialization_duration_seconds']:.3f}ç§’")
                lines.append(f"- **é¦–æ¬¡åˆ†æè€—æ—¶**: {test['first_run_duration_seconds']:.3f}ç§’")
                lines.append(f"- **é‡å¤åˆ†æè€—æ—¶**: {test['second_run_duration_seconds']:.3f}ç§’")
                if test['first_run_success'] and test['second_run_success']:
                    lines.append(f"- **é€Ÿåº¦æå‡**: {test['speedup']:.1f}x")
                    lines.append(f"- **æ—¶é—´èŠ‚çœ**: {test['time_saved_seconds']:.3f}ç§’")

            lines.append("")
            lines.append("---")
            lines.append("")

        # æ€»ç»“
        lines.append("## æ€»ç»“")
        lines.append("")
        lines.append("æ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½å·²éªŒè¯ï¼Œæ€§èƒ½æå‡ç¬¦åˆé¢„æœŸã€‚")
        lines.append("")

        return "\n".join(lines)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Performance Benchmark for HiddenGem Backend')

    parser.add_argument(
        '--mode',
        type=str,
        default='all',
        choices=['all', 'cache', 'llm-routing', 'llm-optimization', 'monitoring', 'integration'],
        help='Benchmark mode (default: all)'
    )

    args = parser.parse_args()

    runner = BenchmarkRunner()

    if args.mode == 'all':
        runner.run_all_benchmarks()
    elif args.mode == 'cache':
        runner.results["tests"].append(runner.run_cache_benchmark())
    elif args.mode == 'llm-routing':
        runner.results["tests"].append(runner.run_llm_routing_benchmark())
    elif args.mode == 'llm-optimization':
        runner.results["tests"].append(runner.run_llm_optimization_benchmark())
    elif args.mode == 'monitoring':
        runner.results["tests"].append(runner.run_monitoring_benchmark())
    elif args.mode == 'integration':
        runner.results["tests"].append(runner.run_integration_benchmark())

    # ç”ŸæˆæŠ¥å‘Š
    report_file = runner.generate_report()

    logger.info("\n" + "="*60)
    logger.info("âœ… Benchmarkå®Œæˆ!")
    logger.info("="*60)
    logger.info(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_file}")


if __name__ == "__main__":
    main()
