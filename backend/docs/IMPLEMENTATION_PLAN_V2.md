# HiddenGem å®æ–½è®¡åˆ’ v2.0 - åŸºäºå¼€æºæ¡†æ¶

## æ ¸å¿ƒç†å¿µï¼šç«™åœ¨å·¨äººçš„è‚©è†€ä¸Š

**ä¸è¦é‡å¤é€ è½®å­ï¼ä½¿ç”¨æˆç†Ÿçš„å¼€æºé¡¹ç›®ä½œä¸ºåŸºç¡€ã€‚**

## æŠ€æœ¯é€‰å‹å¯¹æ¯”

### é€‰é¡¹1: FinRL Framework â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ (å¼ºçƒˆæ¨è)

**ä¼˜åŠ¿**ï¼š
- âœ… **å®Œæ•´çš„RLäº¤æ˜“æ¡†æ¶**ï¼šå·²å®ç°Environmentã€Agentã€Trainer
- âœ… **åŸç”Ÿæ”¯æŒPPO/DDPG/SAC**ï¼šåŸºäºStable-Baselines3
- âœ… **å¤šå¸‚åœºæ”¯æŒ**ï¼šAè‚¡ã€ç¾è‚¡ã€åŠ å¯†è´§å¸
- âœ… **é£é™©çº¦æŸ**ï¼šå·²æœ‰CVaR-PPOå®ç°ï¼ˆæ­£æ˜¯æˆ‘ä»¬éœ€è¦çš„ï¼ï¼‰
- âœ… **æ€§èƒ½æŒ‡æ ‡**ï¼šSharpeã€Sortinoã€MaxDDã€Calmarå…¨éƒ½æœ‰
- âœ… **æ´»è·ƒç»´æŠ¤**ï¼šGitHub 9k+ starsï¼ŒæŒç»­æ›´æ–°

**åŠ£åŠ¿**ï¼š
- âš ï¸ å­¦ä¹ æ›²çº¿ï¼šéœ€è¦ç†è§£FinRLçš„æ¶æ„
- âš ï¸ å®šåˆ¶æ€§ï¼šéœ€è¦é€‚é…æˆ‘ä»¬çš„TradingAgentsä¿¡å·

**æˆ‘ä»¬éœ€è¦åšçš„**ï¼š
1. æ‰©å±•FinRLçš„State Spaceï¼Œæ·»åŠ TradingAgentsä¿¡å·ä½œä¸ºç‰¹å¾
2. è‡ªå®šä¹‰Reward Functionï¼Œæ•´åˆæˆ‘ä»¬çš„é£é™©åå¥½
3. é›†æˆè®°å¿†ç³»ç»Ÿï¼Œå®ç°ç»éªŒå›æ”¾
4. é€‚é…Aè‚¡æ•°æ®æºï¼ˆTushareï¼‰

**å·¥ä½œé‡**ï¼š2-3å‘¨ï¼ˆvs è‡ªå·±å®ç°çš„12-17å‘¨ï¼‰

---

### é€‰é¡¹2: Backtrader + Stable-Baselines3 â­ï¸â­ï¸â­ï¸â­ï¸

**ä¼˜åŠ¿**ï¼š
- âœ… **Backtrader**ï¼šæœ€æµè¡Œçš„Pythonå›æµ‹æ¡†æ¶
- âœ… **çµæ´»æ€§é«˜**ï¼šæ”¯æŒå„ç§è‡ªå®šä¹‰ç­–ç•¥
- âœ… **ç”Ÿæ€ä¸°å¯Œ**ï¼šå¤§é‡ç¤ºä¾‹å’Œæ’ä»¶
- âœ… **SB3**ï¼šæˆç†Ÿçš„RLåº“ï¼Œæ”¯æŒå¤šç§ç®—æ³•

**åŠ£åŠ¿**ï¼š
- âš ï¸ éœ€è¦è‡ªå·±æ¡¥æ¥Backtraderå’ŒSB3
- âš ï¸ éœ€è¦è‡ªå·±å®ç°Gymæ¥å£

**å·¥ä½œé‡**ï¼š3-4å‘¨

---

### é€‰é¡¹3: è‡ªå·±å®ç° â­ï¸â­ï¸

**ä¼˜åŠ¿**ï¼š
- âœ… å®Œå…¨æ§åˆ¶
- âœ… å®šåˆ¶åŒ–

**åŠ£åŠ¿**ï¼š
- âŒ æ—¶é—´æˆæœ¬é«˜ï¼ˆ12-17å‘¨ï¼‰
- âŒ Bugå¤šï¼Œéœ€è¦å¤§é‡æµ‹è¯•
- âŒ é‡å¤é€ è½®å­

**å·¥ä½œé‡**ï¼š12-17å‘¨

---

## æœ€ç»ˆé€‰æ‹©ï¼šFinRL + TradingAgents

ç»¼åˆè€ƒè™‘ï¼Œæˆ‘ä»¬é€‰æ‹© **FinRL** ä½œä¸ºåŸºç¡€æ¡†æ¶ï¼ŒåŸå› ï¼š

1. **FinRL-DeepSeekè®ºæ–‡**æœ¬èº«å°±æ˜¯åŸºäºFinRLå®ç°çš„
2. å·²æœ‰CVaR-PPOå®ç°ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨æˆ–å¾®è°ƒ
3. å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹ï¼Œå¼€ç®±å³ç”¨
4. ç¤¾åŒºæ´»è·ƒï¼Œé‡åˆ°é—®é¢˜å®¹æ˜“æ‰¾åˆ°è§£å†³æ–¹æ¡ˆ

## ä¿®è®¢åçš„å®æ–½è®¡åˆ’

### Phase 1: ç¯å¢ƒå‡†å¤‡ (1å‘¨)

#### Task 1.1: å®‰è£…FinRL

```bash
# å®‰è£…FinRL
pip install git+https://github.com/AI4Finance-Foundation/FinRL.git

# å®‰è£…ä¾èµ–
pip install stable-baselines3[extra]
pip install gym
pip install pyfolio
```

#### Task 1.2: ç†Ÿæ‚‰FinRLæ¶æ„

**å­¦ä¹ èµ„æº**ï¼š
- FinRLå®˜æ–¹æ–‡æ¡£
- FinRL-DeepSeekè®ºæ–‡å®ç°
- å®˜æ–¹ç¤ºä¾‹ä»£ç 

**å…³é”®ç»„ä»¶**ï¼š
```python
from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv
from finrl.agents.stablebaselines3.models import DRLAgent
from finrl.meta.preprocessor.yahoodownloader import YahooDownloader
from finrl.meta.preprocessor.preprocessors import FeatureEngineer
```

---

### Phase 2: é›†æˆTradingAgentsä¿¡å· (2å‘¨)

#### Task 2.1: æ‰©å±•FinRLçš„State Space

**æ–‡ä»¶**: `backend/tradingagents/rl/enhanced_trading_env.py`

```python
from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv
import numpy as np

class LLMEnhancedTradingEnv(StockTradingEnv):
    """æ‰©å±•FinRLç¯å¢ƒï¼Œæ·»åŠ TradingAgentsä¿¡å·"""

    def __init__(
        self,
        df,
        trading_graph,  # æˆ‘ä»¬çš„TradingAgentsGraph
        memory_manager,  # è®°å¿†ç³»ç»Ÿ
        **kwargs
    ):
        super().__init__(df, **kwargs)
        self.trading_graph = trading_graph
        self.memory_manager = memory_manager

    def _get_observation(self):
        """é‡å†™è§‚å¯Ÿå‡½æ•°ï¼Œæ·»åŠ LLMä¿¡å·"""

        # 1. è·å–åŸå§‹FinRLçš„è§‚å¯Ÿï¼ˆä»·æ ¼ã€æŠ€æœ¯æŒ‡æ ‡ç­‰ï¼‰
        base_obs = super()._get_observation()

        # 2. è·å–å½“å‰æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç 
        current_date = self.df.iloc[self.current_step]['date']
        symbol = self.df.iloc[self.current_step]['tic']

        # 3. è°ƒç”¨TradingAgentsè·å–LLMä¿¡å·
        llm_signals = self._get_llm_signals(symbol, current_date)

        # 4. ä»è®°å¿†ç³»ç»Ÿæ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹
        memory_signals = self._get_memory_signals(symbol, current_date)

        # 5. åˆå¹¶æ‰€æœ‰ä¿¡å·
        enhanced_obs = np.concatenate([
            base_obs,
            llm_signals,
            memory_signals
        ])

        return enhanced_obs

    def _get_llm_signals(self, symbol, date):
        """è·å–TradingAgentsçš„LLMä¿¡å·"""
        # è°ƒç”¨æˆ‘ä»¬å·²æœ‰çš„TradingAgentsç³»ç»Ÿ
        final_state, processed_signal = self.trading_graph.propagate(symbol, date)

        # æå–å…³é”®ä¿¡å·
        llm_analysis = final_state.get('llm_analysis', {})

        signals = np.array([
            self._encode_direction(llm_analysis.get('recommended_direction', 'hold')),
            llm_analysis.get('confidence', 0.5),
            llm_analysis.get('risk_score', 0.5),
            self._calculate_agent_agreement(final_state),
        ])

        return signals

    def _get_memory_signals(self, symbol, date):
        """ä»è®°å¿†ç³»ç»Ÿè·å–ä¿¡å·"""
        # æ£€ç´¢ç›¸ä¼¼å†å²æ¡ˆä¾‹
        similar_episodes = self.memory_manager.retrieve_episodes(
            query_context={'symbol': symbol, 'date': date},
            top_k=5
        )

        if len(similar_episodes) == 0:
            return np.array([0, 0])

        # ç»Ÿè®¡ç›¸ä¼¼æ¡ˆä¾‹çš„å¹³å‡æ”¶ç›Šå’ŒæˆåŠŸç‡
        avg_return = np.mean([ep.outcome.percentage_return for ep in similar_episodes if ep.outcome])
        success_rate = np.mean([1 if ep.success else 0 for ep in similar_episodes])

        return np.array([avg_return, success_rate])

    def _encode_direction(self, direction: str) -> float:
        """ç¼–ç æ–¹å‘ï¼šlong=1, hold=0, short=-1"""
        mapping = {'long': 1.0, 'hold': 0.0, 'short': -1.0}
        return mapping.get(direction, 0.0)

    def _calculate_agent_agreement(self, final_state):
        """è®¡ç®—Agentä¸€è‡´æ€§"""
        agent_results = final_state.get('agent_results', {})
        if len(agent_results) == 0:
            return 0.5

        directions = [r['direction'] for r in agent_results.values()]
        from collections import Counter
        counter = Counter(directions)
        most_common_count = counter.most_common(1)[0][1]

        return most_common_count / len(directions)
```

**é¢„è®¡æ—¶é—´**: 5å¤©

#### Task 2.2: è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

**æ–‡ä»¶**: `backend/tradingagents/rl/custom_reward.py`

```python
def calculate_reward_with_cvar(
    portfolio_value_change,
    actions,
    turbulence,
    cost,
    cvar_alpha=0.95,
    risk_penalty=0.1
):
    """
    è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼ˆåŸºäºFinRL + CVaRçº¦æŸï¼‰

    Args:
        portfolio_value_change: æŠ•èµ„ç»„åˆä»·å€¼å˜åŒ–
        actions: é‡‡å–çš„åŠ¨ä½œ
        turbulence: å¸‚åœºæ³¢åŠ¨ç‡
        cost: äº¤æ˜“æˆæœ¬
        cvar_alpha: CVaRé˜ˆå€¼
        risk_penalty: é£é™©æƒ©ç½šç³»æ•°
    """

    # 1. åŸºç¡€æ”¶ç›Šå¥–åŠ±
    profit_reward = portfolio_value_change

    # 2. CVaRé£é™©æƒ©ç½šï¼ˆå…³æ³¨æç«¯æŸå¤±ï¼‰
    cvar_penalty = 0
    if portfolio_value_change < 0:
        # è®¡ç®—CVaRï¼ˆæœ€å·®5%æƒ…å†µçš„å¹³å‡æŸå¤±ï¼‰
        # è¿™é‡Œéœ€è¦ç»´æŠ¤ä¸€ä¸ªæ»‘åŠ¨çª—å£çš„æ”¶ç›Šç‡å†å²
        cvar_penalty = calculate_cvar(portfolio_value_change, alpha=cvar_alpha)

    # 3. å¸‚åœºæ³¢åŠ¨æƒ©ç½š
    turbulence_penalty = 0
    if turbulence > 1.5:  # å¸‚åœºæåº¦æ³¢åŠ¨
        turbulence_penalty = (turbulence - 1.5) * abs(np.sum(actions))

    # 4. ç»¼åˆå¥–åŠ±
    reward = (
        profit_reward
        - risk_penalty * cvar_penalty
        - turbulence_penalty
        - cost
    )

    return reward
```

**é¢„è®¡æ—¶é—´**: 3å¤©

#### Task 2.3: æ•°æ®å‡†å¤‡

**æ–‡ä»¶**: `backend/tradingagents/rl/data_preparation.py`

```python
from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split
import pandas as pd

def prepare_data_for_training(
    symbol: str,
    start_date: str,
    end_date: str,
    time_interval: str = '1D'
):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""

    # 1. ä½¿ç”¨æˆ‘ä»¬å·²æœ‰çš„æ•°æ®æ¥å£è·å–æ•°æ®
    from tradingagents.dataflows.interface import get_stock_data_by_market

    data = get_stock_data_by_market(symbol, start_date, end_date)

    # 2. è½¬æ¢ä¸ºFinRLæ ¼å¼
    df = pd.DataFrame({
        'date': data.index,
        'tic': symbol,
        'close': data['close'],
        'high': data['high'],
        'low': data['low'],
        'open': data['open'],
        'volume': data['volume'],
    })

    # 3. æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ï¼ˆä½¿ç”¨FinRLçš„FeatureEngineerï¼‰
    fe = FeatureEngineer(
        use_technical_indicator=True,
        tech_indicator_list=[
            'macd', 'rsi_30', 'cci_30', 'dx_30',
            'close_30_sma', 'close_60_sma'
        ],
        use_turbulence=True,
        user_defined_feature=False
    )

    df = fe.preprocess_data(df)

    # 4. æ•°æ®åˆ†å‰²ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•ï¼‰
    train = data_split(df, start_date, '2022-12-31')
    val = data_split(df, '2023-01-01', '2023-12-31')
    test = data_split(df, '2024-01-01', end_date)

    return train, val, test
```

**é¢„è®¡æ—¶é—´**: 2å¤©

---

### Phase 3: è®­ç»ƒRL Agent (2å‘¨)

#### Task 3.1: é…ç½®è®­ç»ƒå‚æ•°

**æ–‡ä»¶**: `backend/config/rl_config.py`

```python
RL_CONFIG = {
    # ç¯å¢ƒé…ç½®
    'initial_amount': 100000,  # åˆå§‹èµ„é‡‘
    'buy_cost_pct': 0.001,     # ä¹°å…¥æ‰‹ç»­è´¹0.1%
    'sell_cost_pct': 0.001,    # å–å‡ºæ‰‹ç»­è´¹0.1%
    'hmax': 100,               # æœ€å¤§æŒä»“æ•°é‡
    'discrete_actions': True,   # ä½¿ç”¨ç¦»æ•£åŠ¨ä½œç©ºé—´

    # RLç®—æ³•é…ç½®ï¼ˆPPOï¼‰
    'model_name': 'ppo',
    'policy': 'MlpPolicy',
    'learning_rate': 0.0003,
    'n_steps': 2048,
    'batch_size': 64,
    'n_epochs': 10,
    'gamma': 0.99,             # æŠ˜æ‰£å› å­
    'ent_coef': 0.01,          # ç†µç³»æ•°ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰

    # CVaRé…ç½®
    'cvar_alpha': 0.95,        # CVaRé˜ˆå€¼
    'risk_penalty': 0.1,       # é£é™©æƒ©ç½šç³»æ•°

    # è®­ç»ƒé…ç½®
    'total_timesteps': 100000,
    'eval_freq': 1000,
    'save_freq': 5000,
}
```

#### Task 3.2: è®­ç»ƒè„šæœ¬

**æ–‡ä»¶**: `backend/scripts/train_rl_with_finrl.py`

```python
from finrl.agents.stablebaselines3.models import DRLAgent
from tradingagents.rl.enhanced_trading_env import LLMEnhancedTradingEnv
from tradingagents.rl.data_preparation import prepare_data_for_training
from tradingagents.graph.trading_graph import TradingAgentsGraph
from memory import MemoryManager, MemoryMode
from config.rl_config import RL_CONFIG

def train_rl_agent(symbol: str, start_date: str, end_date: str):
    """è®­ç»ƒRL Agent"""

    # 1. å‡†å¤‡æ•°æ®
    print("ğŸ“Š å‡†å¤‡æ•°æ®...")
    train_data, val_data, test_data = prepare_data_for_training(
        symbol, start_date, end_date
    )

    # 2. åˆå§‹åŒ–TradingAgents
    print("ğŸ¤– åˆå§‹åŒ–TradingAgents...")
    trading_graph = TradingAgentsGraph()

    # 3. åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿï¼ˆè®­ç»ƒæ¨¡å¼ï¼šè¯»å†™ï¼‰
    print("ğŸ“š åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ...")
    memory_manager = MemoryManager(
        mode=MemoryMode.TRAINING,
        config=DEFAULT_CONFIG
    )

    # 4. åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    print("ğŸ—ï¸ åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    env_train = LLMEnhancedTradingEnv(
        df=train_data,
        trading_graph=trading_graph,
        memory_manager=memory_manager,
        **RL_CONFIG
    )

    # 5. åˆ›å»ºRL Agentï¼ˆä½¿ç”¨FinRLçš„DRLAgentï¼‰
    print("ğŸ§  åˆ›å»ºRL Agent...")
    agent = DRLAgent(env=env_train)

    # 6. è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    model = agent.get_model(
        model_name=RL_CONFIG['model_name'],
        model_kwargs={
            'policy': RL_CONFIG['policy'],
            'learning_rate': RL_CONFIG['learning_rate'],
            'n_steps': RL_CONFIG['n_steps'],
            'batch_size': RL_CONFIG['batch_size'],
            'n_epochs': RL_CONFIG['n_epochs'],
            'gamma': RL_CONFIG['gamma'],
            'ent_coef': RL_CONFIG['ent_coef'],
        }
    )

    trained_model = agent.train_model(
        model=model,
        tb_log_name='ppo_trading',
        total_timesteps=RL_CONFIG['total_timesteps']
    )

    # 7. ä¿å­˜æ¨¡å‹
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
    trained_model.save(f"models/rl_agent_{symbol}")

    # 8. éªŒè¯é›†è¯„ä¼°
    print("ğŸ“ˆ éªŒè¯é›†è¯„ä¼°...")
    env_val = LLMEnhancedTradingEnv(
        df=val_data,
        trading_graph=trading_graph,
        memory_manager=memory_manager,
        **RL_CONFIG
    )

    val_results = DRLAgent.DRL_prediction(
        model=trained_model,
        environment=env_val
    )

    print(f"âœ… è®­ç»ƒå®Œæˆï¼éªŒè¯é›†æ”¶ç›Š: {val_results['total_return']:.2%}")

    return trained_model

if __name__ == "__main__":
    train_rl_agent(
        symbol='600519.SH',
        start_date='2018-01-01',
        end_date='2024-12-31'
    )
```

**é¢„è®¡æ—¶é—´**: 5å¤©

#### Task 3.3: è¯„ä¼°å’Œå›æµ‹

**æ–‡ä»¶**: `backend/scripts/evaluate_rl_agent.py`

```python
from finrl.plot import backtest_stats, backtest_plot, get_baseline
import pyfolio

def evaluate_rl_agent(model, test_data, symbol):
    """è¯„ä¼°RL Agentæ€§èƒ½"""

    # 1. åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œ
    print("ğŸ“Š æµ‹è¯•é›†è¯„ä¼°...")
    env_test = LLMEnhancedTradingEnv(df=test_data, **RL_CONFIG)
    df_account_value, df_actions = DRLAgent.DRL_prediction(
        model=model,
        environment=env_test
    )

    # 2. è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    print("ğŸ“ˆ è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
    perf_stats = backtest_stats(
        account_value=df_account_value,
        value_col_name='account_value'
    )

    # 3. ä¸åŸºå‡†å¯¹æ¯”ï¼ˆä¹°å…¥æŒæœ‰ç­–ç•¥ï¼‰
    print("ğŸ“Š åŸºå‡†å¯¹æ¯”...")
    baseline_df = get_baseline(
        ticker=symbol,
        start=test_data['date'].min(),
        end=test_data['date'].max()
    )

    # 4. ç»˜åˆ¶ç»“æœ
    print("ğŸ“‰ ç»˜åˆ¶å›æµ‹æ›²çº¿...")
    backtest_plot(
        df_account_value,
        baseline_df=baseline_df,
        baseline_ticker=symbol
    )

    # 5. PyFolioè¯¦ç»†åˆ†æ
    print("ğŸ“Š PyFolioåˆ†æ...")
    returns = df_account_value['daily_return']
    pyfolio.create_full_tear_sheet(returns)

    return perf_stats
```

**é¢„è®¡æ—¶é—´**: 3å¤©

---

### Phase 4: éƒ¨ç½²å’Œç›‘æ§ (1å‘¨)

#### Task 4.1: æ¨¡æ‹Ÿäº¤æ˜“API

**æ–‡ä»¶**: `backend/api/rl_trading_router.py`

```python
from fastapi import APIRouter
from stable_baselines3 import PPO

router = APIRouter(prefix="/api/v1/rl", tags=["RL Trading"])

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
rl_model = PPO.load("models/rl_agent_600519.SH")

@router.post("/predict/{symbol}")
async def predict_action(symbol: str):
    """é¢„æµ‹ä¸‹ä¸€æ­¥åŠ¨ä½œ"""

    # 1. è·å–å½“å‰çŠ¶æ€
    current_state = get_current_state(symbol)

    # 2. RLæ¨¡å‹é¢„æµ‹
    action, _states = rl_model.predict(current_state, deterministic=True)

    # 3. è§£ç åŠ¨ä½œ
    decoded_action = decode_action(action)

    return {
        "success": True,
        "data": {
            "symbol": symbol,
            "action": decoded_action['type'],  # BUY/SELL/HOLD
            "size": decoded_action['size'],     # ä»“ä½å¤§å°
            "confidence": _states['value_estimate']
        }
    }
```

**é¢„è®¡æ—¶é—´**: 3å¤©

---

## ä¿®è®¢åçš„æ—¶é—´ä¼°ç®—

- **Phase 1 (ç¯å¢ƒå‡†å¤‡)**: 1å‘¨
- **Phase 2 (é›†æˆTradingAgents)**: 2å‘¨
- **Phase 3 (è®­ç»ƒRL Agent)**: 2å‘¨
- **Phase 4 (éƒ¨ç½²ç›‘æ§)**: 1å‘¨

**æ€»è®¡**: 6å‘¨ï¼ˆvs åŸè®¡åˆ’çš„12-17å‘¨ï¼ŒèŠ‚çœ50%+æ—¶é—´ï¼‰

## å…³é”®ä¼˜åŠ¿æ€»ç»“

âœ… **ä½¿ç”¨FinRLåçš„ä¼˜åŠ¿**ï¼š
1. èŠ‚çœ10å‘¨å¼€å‘æ—¶é—´
2. ä»£ç è´¨é‡æ›´é«˜ï¼ˆä¹…ç»è€ƒéªŒï¼‰
3. æ€§èƒ½è¯„ä¼°æ›´ä¸“ä¸šï¼ˆPyFolioé›†æˆï¼‰
4. ç¤¾åŒºæ”¯æŒï¼ˆé‡åˆ°é—®é¢˜å®¹æ˜“è§£å†³ï¼‰
5. å·²æœ‰CVaR-PPOå®ç°ï¼ˆæ­£æ˜¯æˆ‘ä»¬éœ€è¦çš„ï¼‰

âœ… **æˆ‘ä»¬çš„åˆ›æ–°ç‚¹**ï¼š
1. TradingAgentsçš„å¤šAgent LLMä¿¡å·ï¼ˆFinRLåŸç‰ˆæ²¡æœ‰ï¼‰
2. åŒå±‚è®°å¿†ç³»ç»Ÿé›†æˆï¼ˆç‹¬åˆ›ï¼‰
3. æ—¶é—´æ—…è¡Œè®­ç»ƒä¸RLè®­ç»ƒç»“åˆï¼ˆåˆ›æ–°ï¼‰

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³å¼€å§‹**: Phase 1 - å®‰è£…FinRLå¹¶ç†Ÿæ‚‰æ¶æ„
2. **é˜…è¯»èµ„æ–™**:
   - FinRLå®˜æ–¹æ–‡æ¡£
   - FinRL-DeepSeekè®ºæ–‡çš„ä»£ç å®ç°
3. **éªŒè¯å¯è¡Œæ€§**: è¿è¡ŒFinRLå®˜æ–¹ç¤ºä¾‹

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0
**æœ€åæ›´æ–°**: 2025-01-09
**æ ¸å¿ƒå˜åŒ–**: ä»è‡ªå·±å®ç°è½¬å‘ä½¿ç”¨FinRLæ¡†æ¶ï¼ŒèŠ‚çœ50%+å¼€å‘æ—¶é—´
